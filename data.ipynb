{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd005298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MIGRATION START ===\n",
      "==> Building user registries (weekly_plans: type=registry)\n",
      "   upserted/modified registries: 3/0\n",
      "==> Building user_days documents from logs/reflections/notes/daily targets\n",
      "   unique user-days to build: 42\n",
      "   user_days migration complete.\n",
      "==> Converting existing weekly plans to 'type=plan' format\n",
      "   converted plan docs: 17\n",
      "==> Ensuring indexes\n",
      "   indexes ensured.\n",
      "=== MIGRATION COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "# migrate_to_two_collections.py\n",
    "# ----------------------------------------------------\n",
    "# One-time migration from:\n",
    "#   - logs, users, goals, weekly_plans (old), reflections\n",
    "# to:\n",
    "#   - weekly_plans (registry + plan docs)\n",
    "#   - user_days (per-day rollups)\n",
    "#\n",
    "# Run:\n",
    "#   $ export MONGO_URI=\"mongodb+srv://...\"\n",
    "#   $ python migrate_to_two_collections.py\n",
    "#\n",
    "# Safe to re-run: uses upserts and idempotent writes.\n",
    "\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime, timedelta, date\n",
    "from collections import defaultdict\n",
    "\n",
    "from pymongo import MongoClient, UpdateOne, ASCENDING, DESCENDING\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "MONGO_URI = \"mongodb+srv://prashanth01071995:pradsml%402025@cluster0.fsbic.mongodb.net/\"  # <-- set ENV or hardcode a URI string here\n",
    "DB_NAME = \"time_tracker_db\"\n",
    "DEEP_WORK_MIN = 23  # >=23m counts as deep-work session\n",
    "DEFAULT_CATS = [\"Learning\", \"Projects\", \"Research\", \"Planning\"]\n",
    "\n",
    "if not MONGO_URI:\n",
    "    raise SystemExit(\"Set MONGO_URI env var (or hardcode in script).\")\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "\n",
    "logs_col         = db[\"logs\"]\n",
    "users_col        = db[\"users\"]\n",
    "goals_col        = db[\"goals\"]\n",
    "old_plans_col    = db[\"weekly_plans\"]\n",
    "reflections_col  = db[\"reflections\"]\n",
    "\n",
    "# New target collections\n",
    "weekly_plans_col = db[\"weekly_plans\"]   # reused name, but structure changes (type=registry/plan)\n",
    "user_days_col    = db[\"user_days\"]\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "IST_OFFSET_MIN = 330  # for simple \"minutes from midnight\" stability calc\n",
    "\n",
    "def week_bounds_ist(d: date):\n",
    "    \"\"\"Return Monday-start week [start, end] for a given date (naive).\"\"\"\n",
    "    weekday = d.weekday()  # Mon=0\n",
    "    start = d - timedelta(days=weekday)\n",
    "    end = start + timedelta(days=6)\n",
    "    return start, end\n",
    "\n",
    "def parse_time_to_minutes(tstr: str):\n",
    "    \"\"\"Parse '09:34 PM' to minutes from midnight (0..1439). Returns None if invalid.\"\"\"\n",
    "    try:\n",
    "        dt = datetime.strptime(tstr, \"%I:%M %p\")\n",
    "        return dt.hour * 60 + dt.minute\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def clamp_priority_band(raw):\n",
    "    \"\"\"Normalize priority weight to a 1..3 band.\"\"\"\n",
    "    try:\n",
    "        v = int(raw)\n",
    "        if v <= 1: return 1\n",
    "        if v >= 3: return 3\n",
    "        return v\n",
    "    except Exception:\n",
    "        return 2\n",
    "\n",
    "def daterange(start_date: date, end_date: date):\n",
    "    cur = start_date\n",
    "    while cur <= end_date:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "def iso(d: date) -> str:\n",
    "    return d.isoformat()\n",
    "\n",
    "# -----------------------\n",
    "# 1) Build per-user REGISTRY docs in weekly_plans\n",
    "# -----------------------\n",
    "def migrate_registry():\n",
    "    print(\"==> Building user registries (weekly_plans: type=registry)\")\n",
    "    # Collect all usernames we care about\n",
    "    users = set(u.get(\"username\") for u in users_col.find({}, {\"username\":1})) - {None}\n",
    "    # Also include users referenced in logs/goals as fallback\n",
    "    users |= set(r.get(\"user\") for r in logs_col.find({}, {\"user\":1})) - {None}\n",
    "    users |= set(g.get(\"user\") for g in goals_col.find({}, {\"user\":1})) - {None}\n",
    "\n",
    "    ops = []\n",
    "    for user in sorted(users):\n",
    "        # user defaults\n",
    "        udoc = users_col.find_one({\"username\": user}) or {}\n",
    "        defaults = {\n",
    "            \"weekday_poms\": int(udoc.get(\"weekday_poms\", 3)),\n",
    "            \"weekend_poms\": int(udoc.get(\"weekend_poms\", 5)),\n",
    "            \"auto_break\": bool(udoc.get(\"auto_break\", True)),\n",
    "            \"custom_categories\": list(udoc.get(\"custom_categories\", DEFAULT_CATS)),\n",
    "        }\n",
    "\n",
    "        # consolidate goals\n",
    "        goals_map = {}\n",
    "        for g in goals_col.find({\"user\": user}):\n",
    "            gid = str(g.get(\"_id\"))\n",
    "            pw  = g.get(\"priority_weight\", 2)\n",
    "            goals_map[gid] = {\n",
    "                \"title\": g.get(\"title\", \"(untitled)\"),\n",
    "                \"goal_type\": g.get(\"goal_type\", \"Other\"),\n",
    "                \"priority_weight\": pw,                         # keep original\n",
    "                \"priority_band\": clamp_priority_band(pw),      # 1..3 for UI\n",
    "                \"status\": g.get(\"status\", \"New\"),\n",
    "                \"target_poms\": int(g.get(\"target_poms\", 0) or 0),\n",
    "                \"poms_completed\": int(g.get(\"poms_completed\", 0) or 0),\n",
    "                \"created_at\": g.get(\"created_at\", datetime.utcnow()),\n",
    "                \"updated_at\": g.get(\"updated_at\", datetime.utcnow()),\n",
    "            }\n",
    "\n",
    "        reg_id = f\"{user}|registry\"\n",
    "        doc = {\n",
    "            \"_id\": reg_id,\n",
    "            \"user\": user,\n",
    "            \"type\": \"registry\",\n",
    "            \"user_defaults\": defaults,\n",
    "            \"goals\": goals_map,\n",
    "            \"schema_version\": 2,\n",
    "            \"updated_at\": datetime.utcnow(),\n",
    "        }\n",
    "        # Upsert (idempotent)\n",
    "        ops.append(UpdateOne({\"_id\": reg_id},\n",
    "                             {\"$set\": doc, \"$setOnInsert\": {\"created_at\": datetime.utcnow()}},\n",
    "                             upsert=True))\n",
    "    if ops:\n",
    "        res = weekly_plans_col.bulk_write(ops, ordered=False)\n",
    "        print(f\"   upserted/modified registries: {res.upserted_count}/{res.modified_count}\")\n",
    "    else:\n",
    "        print(\"   no users found to create registries.\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Build USER-DAYS from logs (+ reflections, targets, notes)\n",
    "# -----------------------\n",
    "def migrate_user_days():\n",
    "    print(\"==> Building user_days documents from logs/reflections/notes/daily targets\")\n",
    "\n",
    "    # Gather impacted (user, date) pairs from logs types we care about\n",
    "    # Pomodoro sessions + Notes + DailyTarget\n",
    "    pairs = set()\n",
    "    for r in logs_col.find({\"type\": {\"$in\": [\"Pomodoro\", \"Note\", \"DailyTarget\"]}},\n",
    "                           {\"user\":1, \"date\":1}):\n",
    "        user = r.get(\"user\")\n",
    "        dstr = r.get(\"date\")\n",
    "        if not user or not dstr:\n",
    "            continue\n",
    "        pairs.add((user, dstr))\n",
    "\n",
    "    # Include reflections too\n",
    "    for rf in reflections_col.find({}, {\"user\":1, \"date\":1}):\n",
    "        user = rf.get(\"user\")\n",
    "        dstr = rf.get(\"date\")\n",
    "        if not user or not dstr:\n",
    "            continue\n",
    "        pairs.add((user, dstr))\n",
    "\n",
    "    print(f\"   unique user-days to build: {len(pairs)}\")\n",
    "\n",
    "    ops = []\n",
    "    for (user, dstr) in sorted(pairs):\n",
    "        # pull all items for that user+date\n",
    "        pomos = list(logs_col.find({\"type\":\"Pomodoro\", \"user\": user, \"date\": dstr}))\n",
    "        notes = list(logs_col.find({\"type\":\"Note\", \"user\": user, \"date\": dstr}))\n",
    "        target_doc = logs_col.find_one({\"type\":\"DailyTarget\", \"user\": user, \"date\": dstr})\n",
    "        reflect = reflections_col.find_one({\"user\": user, \"date\": dstr})\n",
    "\n",
    "        sessions = []\n",
    "        last_key = None\n",
    "        switches = 0\n",
    "        start_time_mins = []\n",
    "\n",
    "        work_minutes = work_sessions = 0\n",
    "        break_minutes = break_sessions = 0\n",
    "        deep_work = 0\n",
    "\n",
    "        by_cat_minutes = defaultdict(int)\n",
    "        by_goal_sessions = defaultdict(int)\n",
    "\n",
    "        # sort pomos by time to compute switches and start-time order\n",
    "        def as_minutes(t):\n",
    "            m = parse_time_to_minutes(t or \"\")\n",
    "            return (m if m is not None else 0)\n",
    "        pomos_sorted = sorted(pomos, key=lambda r: as_minutes(r.get(\"time\")))\n",
    "\n",
    "        for r in pomos_sorted:\n",
    "            is_break = (r.get(\"pomodoro_type\",\"Work\") == \"Break\")\n",
    "            dur = int(r.get(\"duration\", 0) or 0)\n",
    "            t   = r.get(\"time\")\n",
    "            goal_id = r.get(\"goal_id\")\n",
    "            task = r.get(\"task\", \"\")\n",
    "            cat = r.get(\"category\") or \"\"\n",
    "\n",
    "            sessions.append({\n",
    "                \"t\": (\"B\" if is_break else \"W\"),\n",
    "                \"dur\": dur,\n",
    "                \"time\": t,\n",
    "                **({\"goal_id\": str(goal_id)} if (goal_id is not None and str(goal_id) != \"None\" and str(goal_id) != \"\") else {}),\n",
    "                **({\"task\": task} if task else {}),\n",
    "                **({\"cat\": cat} if (cat and (goal_id in (None, \"\", \"None\"))) else {}),\n",
    "            })\n",
    "\n",
    "            # aggregates\n",
    "            if is_break:\n",
    "                break_minutes += dur\n",
    "                break_sessions += 1\n",
    "            else:\n",
    "                work_minutes += dur\n",
    "                work_sessions += 1\n",
    "                if dur >= DEEP_WORK_MIN:\n",
    "                    deep_work += 1\n",
    "                if goal_id not in (None, \"\", \"None\"):\n",
    "                    by_goal_sessions[str(goal_id)] += 1\n",
    "                else:\n",
    "                    if cat:\n",
    "                        by_cat_minutes[cat] += dur\n",
    "                    else:\n",
    "                        by_cat_minutes[\"Uncategorized\"] += dur\n",
    "\n",
    "            # switches (only between work contexts)\n",
    "            if not is_break:\n",
    "                key = str(goal_id) if goal_id not in (None, \"\", \"None\") else (\"CAT::\" + (cat or task or \"\"))\n",
    "                if last_key is not None and key != last_key:\n",
    "                    switches += 1\n",
    "                last_key = key\n",
    "\n",
    "            # start time mins (track for work sessions)\n",
    "            mins = parse_time_to_minutes(t or \"\")\n",
    "            if mins is not None and not is_break:\n",
    "                start_time_mins.append(mins)\n",
    "\n",
    "        # reflection + notes + target\n",
    "        reflection = None\n",
    "        if reflect:\n",
    "            reflection = {\n",
    "                \"aligned\": reflect.get(\"aligned\"),\n",
    "                \"focus_rating\": int(reflect.get(\"focus_rating\", 0) or 0),\n",
    "                \"blockers\": reflect.get(\"blockers\", \"\"),\n",
    "                \"notes\": reflect.get(\"notes\", \"\"),\n",
    "            }\n",
    "\n",
    "        notes_arr = []\n",
    "        for n in notes:\n",
    "            notes_arr.append({\"content\": n.get(\"content\", \"\"), \"created_at\": n.get(\"created_at\", datetime.utcnow())})\n",
    "\n",
    "        doc = {\n",
    "            \"_id\": f\"{user}|{dstr}\",\n",
    "            \"user\": user,\n",
    "            \"date\": dstr,\n",
    "            \"sessions\": sessions,\n",
    "            \"totals\": {\n",
    "                \"work_minutes\": work_minutes,\n",
    "                \"work_sessions\": work_sessions,\n",
    "                \"break_minutes\": break_minutes,\n",
    "                \"break_sessions\": break_sessions,\n",
    "                \"deep_work_sessions\": deep_work,\n",
    "            },\n",
    "            \"by_category_minutes\": dict(by_cat_minutes),\n",
    "            \"by_goal_sessions\": dict(by_goal_sessions),\n",
    "            \"start_time_mins\": start_time_mins,\n",
    "            \"switches\": switches,\n",
    "            **({\"daily_target\": int(target_doc.get(\"target\", 0))} if target_doc else {}),\n",
    "            **({\"reflection\": reflection} if reflection else {}),\n",
    "            **({\"notes\": notes_arr} if notes_arr else {}),\n",
    "            \"schema_version\": 2,\n",
    "            \"updated_at\": datetime.utcnow(),\n",
    "        }\n",
    "        ops.append(UpdateOne({\"_id\": doc[\"_id\"]},\n",
    "                             {\"$set\": doc, \"$setOnInsert\": {\"created_at\": datetime.utcnow()}},\n",
    "                             upsert=True))\n",
    "        if len(ops) >= 500:\n",
    "            res = user_days_col.bulk_write(ops, ordered=False)\n",
    "            ops = []\n",
    "    if ops:\n",
    "        res = user_days_col.bulk_write(ops, ordered=False)\n",
    "\n",
    "    print(\"   user_days migration complete.\")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Convert old weekly_plans ‚Üí new PLAN docs with snapshots and stats\n",
    "# -----------------------\n",
    "def migrate_weekly_plans():\n",
    "    print(\"==> Converting existing weekly plans to 'type=plan' format\")\n",
    "    # Build a quick registry cache\n",
    "    registry_cache = { d[\"user\"]: d for d in weekly_plans_col.find({\"type\":\"registry\"}) }\n",
    "\n",
    "    # Old plans had _id=\"user|YYYY-MM-DD\" and fields goals/allocations/total_poms\n",
    "    # We‚Äôll rewrite/augment them to the new structure.\n",
    "    cur = old_plans_col.find({\"type\": {\"$exists\": False}})  # heuristic: only old/plain docs\n",
    "    count = 0\n",
    "    for p in cur:\n",
    "        user = p.get(\"user\")\n",
    "        if not user:\n",
    "            continue\n",
    "        week_start = p.get(\"week_start\")\n",
    "        week_end   = p.get(\"week_end\")\n",
    "        if not week_start or not week_end:\n",
    "            # infer from _id if possible\n",
    "            try:\n",
    "                _, ws = p[\"_id\"].split(\"|\", 1)\n",
    "                week_start = ws\n",
    "                wsd = date.fromisoformat(week_start)\n",
    "                week_end = (wsd + timedelta(days=6)).isoformat()\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        # capacity\n",
    "        total = int(p.get(\"total_poms\", 0))\n",
    "        # If we have registry defaults, snapshot weekday/weekend rates;\n",
    "        # otherwise leave them unknown and compute total only\n",
    "        reg = registry_cache.get(user)\n",
    "        if reg:\n",
    "            weekday = int(reg[\"user_defaults\"].get(\"weekday_poms\", 3))\n",
    "            weekend = int(reg[\"user_defaults\"].get(\"weekend_poms\", 5))\n",
    "        else:\n",
    "            weekday, weekend = 3, 5\n",
    "\n",
    "        # goals snapshot\n",
    "        allocations = p.get(\"allocations\", {}) or {}\n",
    "        embedded = []\n",
    "        for gid, planned in allocations.items():\n",
    "            planned = int(planned or 0)\n",
    "            gmeta = (reg[\"goals\"].get(gid) if (reg and reg.get(\"goals\")) else None)\n",
    "            title = (gmeta.get(\"title\") if gmeta else \"(missing)\")\n",
    "            prio  = (gmeta.get(\"priority_band\", 2) if gmeta else 2)\n",
    "            status= (gmeta.get(\"status\") if gmeta else \"In Progress\")\n",
    "            embedded.append({\n",
    "                \"goal_id\": gid,\n",
    "                \"title\": title,\n",
    "                \"priority_weight\": prio,\n",
    "                \"status_at_plan\": status,\n",
    "                \"planned\": planned,\n",
    "                \"carryover_in\": 0,\n",
    "                \"carryover_out\": 0\n",
    "            })\n",
    "\n",
    "        # compute weekly stats from user_days (preferred) or fallback from logs\n",
    "        # window: Monday..Sunday inclusive\n",
    "        wsd = date.fromisoformat(week_start)\n",
    "        wed = date.fromisoformat(week_end)\n",
    "        # from user_days:\n",
    "        goal_actual = defaultdict(int)\n",
    "        custom_unplanned = 0\n",
    "        break_count = 0\n",
    "        break_minutes_total = 0\n",
    "        deep_work_sessions = 0\n",
    "\n",
    "        for d in daterange(wsd, wed):\n",
    "            daydoc = user_days_col.find_one({\"_id\": f\"{user}|{d.isoformat()}\"})\n",
    "            if not daydoc:\n",
    "                continue\n",
    "            # goals\n",
    "            for gid, cnt in (daydoc.get(\"by_goal_sessions\") or {}).items():\n",
    "                goal_actual[gid] += int(cnt)\n",
    "            # unplanned (work sessions without goal_id):\n",
    "            # Approximate via totals - sum(by_goal_sessions)\n",
    "            gsum = sum((daydoc.get(\"by_goal_sessions\") or {}).values())\n",
    "            custom_unplanned += max(0, int((daydoc.get(\"totals\") or {}).get(\"work_sessions\", 0)) - int(gsum))\n",
    "            # breaks\n",
    "            break_count += int((daydoc.get(\"totals\") or {}).get(\"break_sessions\", 0))\n",
    "            break_minutes_total += int((daydoc.get(\"totals\") or {}).get(\"break_minutes\", 0))\n",
    "            # deep-work\n",
    "            deep_work_sessions += int((daydoc.get(\"totals\") or {}).get(\"deep_work_sessions\", 0))\n",
    "\n",
    "        # break hygiene\n",
    "        expected_breaks = sum(goal_actual.values()) + custom_unplanned\n",
    "        skipped = max(0, expected_breaks - break_count)\n",
    "        extended = max(0, break_count - expected_breaks)\n",
    "        avg_break = (break_minutes_total / break_count) if break_count > 0 else 0.0\n",
    "\n",
    "        stats = {\n",
    "            \"actual_by_goal\": dict(goal_actual),\n",
    "            \"custom_unplanned_sessions\": int(custom_unplanned),\n",
    "            \"breaks\": {\n",
    "                \"count\": int(break_count),\n",
    "                \"avg_min\": float(round(avg_break, 2)),\n",
    "                \"skipped\": int(skipped),\n",
    "                \"extended\": int(extended),\n",
    "            },\n",
    "            \"deep_work_sessions\": int(deep_work_sessions),\n",
    "        }\n",
    "\n",
    "        # Upsert new \"plan\" style doc\n",
    "        plan_doc = {\n",
    "            \"_id\": f\"{user}|{week_start}\",\n",
    "            \"user\": user,\n",
    "            \"type\": \"plan\",\n",
    "            \"week_start\": week_start,\n",
    "            \"week_end\": week_end,\n",
    "            \"capacity\": {\"weekday\": weekday, \"weekend\": weekend, \"total\": total},\n",
    "            \"goals_embedded\": embedded,\n",
    "            \"stats\": stats,\n",
    "            \"schema_version\": 2,\n",
    "            \"updated_at\": datetime.utcnow(),\n",
    "        }\n",
    "        weekly_plans_col.update_one(\n",
    "            {\"_id\": plan_doc[\"_id\"]},\n",
    "            {\"$set\": plan_doc, \"$setOnInsert\": {\"created_at\": datetime.utcnow()}},\n",
    "            upsert=True\n",
    "        )\n",
    "        count += 1\n",
    "    print(f\"   converted plan docs: {count}\")\n",
    "\n",
    "# -----------------------\n",
    "# 4) Indexes\n",
    "# -----------------------\n",
    "def ensure_indexes():\n",
    "    print(\"==> Ensuring indexes\")\n",
    "    # weekly_plans: distinct usage for registry and plan\n",
    "    weekly_plans_col.create_index([(\"user\", ASCENDING), (\"week_start\", ASCENDING)], name=\"user_weekstart\")\n",
    "    weekly_plans_col.create_index([(\"type\", ASCENDING), (\"user\", ASCENDING)], name=\"type_user\")\n",
    "    # user_days: unique per user+date\n",
    "    user_days_col.create_index([(\"user\", ASCENDING), (\"date\", ASCENDING)], name=\"user_date\", unique=True)\n",
    "    print(\"   indexes ensured.\")\n",
    "\n",
    "# -----------------------\n",
    "# 5) (Optional) backup originals\n",
    "# -----------------------\n",
    "def optional_backup():\n",
    "    # If you want simple in-DB backups (lightweight), uncomment below.\n",
    "    # db[\"backup_logs\"].drop();         db[\"backup_logs\"].insert_many(list(logs_col.find({})))\n",
    "    # db[\"backup_users\"].drop();        db[\"backup_users\"].insert_many(list(users_col.find({})))\n",
    "    # db[\"backup_goals\"].drop();        db[\"backup_goals\"].insert_many(list(goals_col.find({})))\n",
    "    # db[\"backup_weekly_plans\"].drop(); db[\"backup_weekly_plans\"].insert_many(list(old_plans_col.find({})))\n",
    "    # db[\"backup_reflections\"].drop();  db[\"backup_reflections\"].insert_many(list(reflections_col.find({})))\n",
    "    pass\n",
    "\n",
    "# -----------------------\n",
    "# Run all\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== MIGRATION START ===\")\n",
    "    # optional_backup()\n",
    "    migrate_registry()\n",
    "    migrate_user_days()\n",
    "    migrate_weekly_plans()\n",
    "    ensure_indexes()\n",
    "    print(\"=== MIGRATION COMPLETE ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81a7721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking indexes and docs...\n",
      "‚úÖ Dropped 0 bad _id indexes from user_days\n",
      "‚úÖ Dropped 0 bad _id indexes from weekly_plans\n",
      "‚úÖ Updated 0 user_days docs to schema_version 2\n",
      "‚úÖ Updated 0 weekly_plans docs to schema_version 2\n",
      "‚ú® Done\n"
     ]
    }
   ],
   "source": [
    "# check_and_clean.py\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "MONGO_URI = \"mongodb+srv://prashanth01071995:pradsml%402025@cluster0.fsbic.mongodb.net/\"\n",
    "DB_NAME = \"time_tracker_db\"\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "user_days = db[\"user_days\"]\n",
    "weekly = db[\"weekly_plans\"]\n",
    "\n",
    "def scrub_bad_id_indexes(col):\n",
    "    dropped = 0\n",
    "    for ix in col.list_indexes():\n",
    "        name = ix.get(\"name\", \"\")\n",
    "        key = dict(ix.get(\"key\", {}))\n",
    "        if key == {\"_id\": 1} and name != \"_id_\":\n",
    "            try:\n",
    "                col.drop_index(name)\n",
    "                dropped += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not drop {name} on {col.name}: {e}\")\n",
    "    return dropped\n",
    "\n",
    "def normalize_user_days():\n",
    "    updated = 0\n",
    "    for d in user_days.find({\"schema_version\": {\"$ne\": 2}}):\n",
    "        user_days.update_one({\"_id\": d[\"_id\"]}, {\"$set\": {\"schema_version\": 2, \"updated_at\": datetime.utcnow()}})\n",
    "        updated += 1\n",
    "    return updated\n",
    "\n",
    "def normalize_weekly():\n",
    "    updated = 0\n",
    "    for d in weekly.find({\"schema_version\": {\"$ne\": 2}}):\n",
    "        weekly.update_one({\"_id\": d[\"_id\"]}, {\"$set\": {\"schema_version\": 2, \"updated_at\": datetime.utcnow()}})\n",
    "        updated += 1\n",
    "    return updated\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîç Checking indexes and docs...\")\n",
    "\n",
    "    d1 = scrub_bad_id_indexes(user_days)\n",
    "    d2 = scrub_bad_id_indexes(weekly)\n",
    "\n",
    "    u1 = normalize_user_days()\n",
    "    u2 = normalize_weekly()\n",
    "\n",
    "    print(f\"‚úÖ Dropped {d1} bad _id indexes from user_days\")\n",
    "    print(f\"‚úÖ Dropped {d2} bad _id indexes from weekly_plans\")\n",
    "    print(f\"‚úÖ Updated {u1} user_days docs to schema_version 2\")\n",
    "    print(f\"‚úÖ Updated {u2} weekly_plans docs to schema_version 2\")\n",
    "    print(\"‚ú® Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ca14dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --uri URI [--db DB] [--fix]\n",
      "                             [--drop-stray-indexes]\n",
      "ipykernel_launcher.py: error: argument --fix: ignored explicit argument '/Users/itc/Library/Jupyter/runtime/kernel-v3b89d0cf03fbed506885b8d7cf4092ca9e1753230.json'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/argparse.py:1866\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1866\u001b[0m     namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError:\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/argparse.py:2079\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   2078\u001b[0m     \u001b[38;5;66;03m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[0;32m-> 2079\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m \u001b[43mconsume_optional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;66;03m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/argparse.py:2001\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[0;34m(start_index)\u001b[0m\n\u001b[1;32m   2000\u001b[0m         msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignored explicit argument \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2001\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, msg \u001b[38;5;241m%\u001b[39m explicit_arg)\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# if there is no explicit argument, try to match the\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# optional's string arguments with the following strings\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;66;03m# if successful, exit the loop\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --fix: ignored explicit argument '/Users/itc/Library/Jupyter/runtime/kernel-v3b89d0cf03fbed506885b8d7cf4092ca9e1753230.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 318\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 318\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 273\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 273\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     client \u001b[38;5;241m=\u001b[39m MongoClient(args\u001b[38;5;241m.\u001b[39muri)\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mparse_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m p\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--drop-stray-indexes\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m\"\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDrop unknown custom indexes (never _id_)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/argparse.py:1833\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1833\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/argparse.py:1869\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1868\u001b[0m         err \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1869\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/argparse.py:2594\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2593\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2594\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/argparse.py:2581\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2581\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   2143\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2145\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/IPython/core/ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[0;32m-> 1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/IPython/core/ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1342\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/IPython/core/ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1185\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1190\u001b[0m ):\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/IPython/core/ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1080\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m   1081\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m   1083\u001b[0m )\n\u001b[1;32m   1085\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1086\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/IPython/core/ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1150\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "DB Health Checker for Time Tracker\n",
    "Run:\n",
    "  python check_db_health.py --uri \"mongodb+srv://...\" [--db time_tracker_db] [--fix] [--drop-stray-indexes]\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from datetime import datetime, timedelta, date\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import OperationFailure\n",
    "\n",
    "ISO_DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "TIME_RE = re.compile(r\"^(0?[1-9]|1[0-2]):[0-5]\\d [AP]M$\")  # 12h \"HH:MM AM/PM\"\n",
    "\n",
    "EXPECTED_INDEXES = {\n",
    "    \"user_days\": [\n",
    "        ((\"user\", 1), (\"date\", 1), \"user_date\"),\n",
    "        ((\"sessions.gid\", 1),         None,       \"sessions_gid\"),\n",
    "        ((\"sessions.linked_gid\", 1),  None,       \"sessions_linked_gid\"),\n",
    "        ((\"sessions.unplanned\", 1),   None,       \"sessions_unplanned\"),\n",
    "        ((\"sessions.cat\", 1),         None,       \"sessions_cat\"),\n",
    "    ],\n",
    "    \"weekly_plans\": [\n",
    "        ((\"user\", 1), (\"type\", 1), \"user_type\"),\n",
    "        ((\"user\", 1), (\"week_start\", 1), \"user_week\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "def week_bounds(d: date) -> Tuple[date, date]:\n",
    "    start = d - timedelta(days=d.weekday())  # Monday\n",
    "    end = start + timedelta(days=6)          # Sunday\n",
    "    return start, end\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--uri\", required=True, help=\"MongoDB connection URI\")\n",
    "    p.add_argument(\"--db\", default=\"time_tracker_db\", help=\"Database name\")\n",
    "    p.add_argument(\"--fix\", action=\"store_true\", help=\"Apply safe fixes (schema_version, type, week_end, capacity)\")\n",
    "    p.add_argument(\"--drop-stray-indexes\", action=\"store_true\", help=\"Drop unknown custom indexes (never _id_)\")\n",
    "    return p.parse_args()\n",
    "\n",
    "def keydict(ixkey) -> Dict[str, int]:\n",
    "    # ixkey is a SON like [(\"field\", 1), ...] or dict-like; normalize\n",
    "    if isinstance(ixkey, dict):\n",
    "        return dict(ixkey)\n",
    "    try:\n",
    "        return {k: v for k, v in ixkey}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def ensure_expected_indexes(col, expected_defs: List[Tuple], create=False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Inspect current indexes against allowlist. Optionally create missing ones.\n",
    "    Returns summary dict.\n",
    "    \"\"\"\n",
    "    current = list(col.list_indexes())\n",
    "    cur_by_name = {ix.get(\"name\"): ix for ix in current}\n",
    "    cur_keysets = {ix.get(\"name\"): keydict(ix.get(\"key\", {})) for ix in current}\n",
    "\n",
    "    missing = []\n",
    "    present = []\n",
    "    for defn in expected_defs:\n",
    "        f1, f2, name = defn\n",
    "        if name in cur_by_name:\n",
    "            present.append(name)\n",
    "        else:\n",
    "            missing.append(name)\n",
    "            if create:\n",
    "                keys = []\n",
    "                if f1: keys.append(f1)\n",
    "                if f2: keys.append(f2)\n",
    "                try:\n",
    "                    col.create_index(keys, name=name)\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Could not create index {name} on {col.name}: {e}\")\n",
    "\n",
    "    # Find stray (unknown) non-_id indexes\n",
    "    stray = []\n",
    "    allow = {n for *_, n in expected_defs} | {\"_id_\"}\n",
    "    for ix in current:\n",
    "        n = ix.get(\"name\")\n",
    "        if n not in allow:\n",
    "            # Keep only if not the default _id_\n",
    "            if keydict(ix.get(\"key\", {})) != {\"_id\": 1}:\n",
    "                stray.append(n)\n",
    "\n",
    "    return {\"present\": present, \"missing\": missing, \"stray\": stray, \"cur_keys\": cur_keysets}\n",
    "\n",
    "def drop_stray_indexes(col, stray_names: List[str]):\n",
    "    dropped = 0\n",
    "    for n in stray_names:\n",
    "        try:\n",
    "            col.drop_index(n)\n",
    "            dropped += 1\n",
    "        except OperationFailure as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Could not drop index {n} on {col.name}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Could not drop index {n} on {col.name}: {e}\")\n",
    "    return dropped\n",
    "\n",
    "def validate_user_days(col, fix=False) -> Dict[str, int]:\n",
    "    counts = {\n",
    "        \"docs\": 0,\n",
    "        \"bad_date_format\": 0,\n",
    "        \"missing_user\": 0,\n",
    "        \"missing_sessions\": 0,\n",
    "        \"bad_t\": 0,\n",
    "        \"bad_dur\": 0,\n",
    "        \"bad_time\": 0,\n",
    "        \"set_schema_v2\": 0,\n",
    "    }\n",
    "    cursor = col.find({}, {\"_id\": 1, \"user\": 1, \"date\": 1, \"sessions\": 1, \"schema_version\": 1})\n",
    "    for d in cursor:\n",
    "        counts[\"docs\"] += 1\n",
    "        did = d.get(\"_id\")\n",
    "        user = d.get(\"user\")\n",
    "        date_str = d.get(\"date\")\n",
    "        sessions = d.get(\"sessions\", [])\n",
    "\n",
    "        if not user:\n",
    "            counts[\"missing_user\"] += 1\n",
    "        if not isinstance(sessions, list):\n",
    "            counts[\"missing_sessions\"] += 1\n",
    "        if not (isinstance(date_str, str) and ISO_DATE_RE.match(date_str)):\n",
    "            counts[\"bad_date_format\"] += 1\n",
    "\n",
    "        # schema_version\n",
    "        if d.get(\"schema_version\") != 2 and fix:\n",
    "            col.update_one({\"_id\": did}, {\"$set\": {\"schema_version\": 2, \"updated_at\": datetime.utcnow()}})\n",
    "            counts[\"set_schema_v2\"] += 1\n",
    "\n",
    "        # session validations\n",
    "        if isinstance(sessions, list):\n",
    "            for s in sessions:\n",
    "                t = s.get(\"t\")\n",
    "                dur = s.get(\"dur\")\n",
    "                tm = s.get(\"time\")\n",
    "                if t not in (\"W\", \"B\"):\n",
    "                    counts[\"bad_t\"] += 1\n",
    "                try:\n",
    "                    dur_i = int(dur)\n",
    "                    if dur_i <= 0 or dur_i > 60:  # allow up to 60 to be lenient\n",
    "                        counts[\"bad_dur\"] += 1\n",
    "                except Exception:\n",
    "                    counts[\"bad_dur\"] += 1\n",
    "                if tm and not TIME_RE.match(str(tm)):\n",
    "                    counts[\"bad_time\"] += 1\n",
    "    return counts\n",
    "\n",
    "def validate_weekly_plans(col, fix=False) -> Dict[str, int]:\n",
    "    counts = {\n",
    "        \"docs\": 0,\n",
    "        \"registries\": 0,\n",
    "        \"plans\": 0,\n",
    "        \"bad_week_dates\": 0,\n",
    "        \"fixed_week_end\": 0,\n",
    "        \"capacity_mismatch\": 0,\n",
    "        \"fixed_capacity\": 0,\n",
    "        \"alloc_over_capacity\": 0,\n",
    "        \"set_schema_v2\": 0,\n",
    "        \"missing_type\": 0,\n",
    "    }\n",
    "    cursor = col.find({})\n",
    "    for d in cursor:\n",
    "        counts[\"docs\"] += 1\n",
    "        did = d.get(\"_id\")\n",
    "        typ = d.get(\"type\")\n",
    "        if typ == \"registry\":\n",
    "            counts[\"registries\"] += 1\n",
    "        elif typ == \"plan\":\n",
    "            counts[\"plans\"] += 1\n",
    "        else:\n",
    "            counts[\"missing_type\"] += 1\n",
    "            if fix:\n",
    "                # Decide based on _id pattern: user|YYYY-MM-DD likely plan, else registry\n",
    "                if isinstance(did, str) and re.search(r\"\\|\\d{4}-\\d{2}-\\d{2}$\", did):\n",
    "                    col.update_one({\"_id\": did}, {\"$set\": {\"type\": \"plan\", \"updated_at\": datetime.utcnow()}})\n",
    "                else:\n",
    "                    col.update_one({\"_id\": did}, {\"$set\": {\"type\": \"registry\", \"updated_at\": datetime.utcnow()}})\n",
    "\n",
    "        # schema_version\n",
    "        if d.get(\"schema_version\") != 2 and fix:\n",
    "            col.update_one({\"_id\": did}, {\"$set\": {\"schema_version\": 2, \"updated_at\": datetime.utcnow()}})\n",
    "            counts[\"set_schema_v2\"] += 1\n",
    "\n",
    "        if typ == \"plan\":\n",
    "            ws = d.get(\"week_start\")\n",
    "            we = d.get(\"week_end\")\n",
    "            cap = (d.get(\"capacity\") or {})\n",
    "            weekday = int(cap.get(\"weekday\", 0))\n",
    "            weekend = int(cap.get(\"weekend\", 0))\n",
    "            total = int(cap.get(\"total\", 0))\n",
    "\n",
    "            # Week date validity\n",
    "            ok_dates = True\n",
    "            if not (isinstance(ws, str) and ISO_DATE_RE.match(ws) and isinstance(we, str) and ISO_DATE_RE.match(we)):\n",
    "                ok_dates = False\n",
    "            else:\n",
    "                try:\n",
    "                    ws_d = date.fromisoformat(ws)\n",
    "                    we_d = date.fromisoformat(we)\n",
    "                    ws_calc, we_calc = week_bounds(ws_d)\n",
    "                    if we_d != we_calc:\n",
    "                        counts[\"bad_week_dates\"] += 1\n",
    "                        ok_dates = False\n",
    "                        if fix:\n",
    "                            col.update_one({\"_id\": did}, {\"$set\": {\"week_end\": we_calc.isoformat(), \"updated_at\": datetime.utcnow()}})\n",
    "                            counts[\"fixed_week_end\"] += 1\n",
    "                except Exception:\n",
    "                    ok_dates = False\n",
    "                    counts[\"bad_week_dates\"] += 1\n",
    "\n",
    "            # Capacity math: total == weekday*wdays + weekend*wendays\n",
    "            if ok_dates:\n",
    "                wd_cnt = 5\n",
    "                we_cnt = 2\n",
    "                expected_total = weekday * wd_cnt + weekend * we_cnt\n",
    "                if total != expected_total:\n",
    "                    counts[\"capacity_mismatch\"] += 1\n",
    "                    if fix:\n",
    "                        col.update_one({\"_id\": did}, {\"$set\": {\"capacity.total\": int(expected_total), \"updated_at\": datetime.utcnow()}})\n",
    "                        counts[\"fixed_capacity\"] += 1\n",
    "\n",
    "            # Allocations don‚Äôt exceed capacity total\n",
    "            alloc = (d.get(\"allocations\") or {})\n",
    "            try:\n",
    "                alloc_sum = sum(int(v) for v in alloc.values())\n",
    "            except Exception:\n",
    "                alloc_sum = 0\n",
    "            if total and alloc_sum > total:\n",
    "                counts[\"alloc_over_capacity\"] += 1\n",
    "    return counts\n",
    "\n",
    "def collect_registry_goals(col) -> Dict[str, set]:\n",
    "    \"\"\"\n",
    "    Returns mapping user -> set(goal_ids) from registry docs.\n",
    "    \"\"\"\n",
    "    mapping: Dict[str, set] = {}\n",
    "    for d in col.find({\"type\": \"registry\"}, {\"user\": 1, \"goals\": 1}):\n",
    "        user = d.get(\"user\")\n",
    "        if not user:\n",
    "            continue\n",
    "        goals = d.get(\"goals\") or {}\n",
    "        mapping[user] = set(goals.keys())\n",
    "    return mapping\n",
    "\n",
    "def validate_referential(user_days_col, weekly_plans_col) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Verify that any session with gid/linked_gid points to an existing registry goal for that user.\n",
    "    \"\"\"\n",
    "    counts = {\"sessions_with_gid\": 0, \"missing_goal_refs\": 0}\n",
    "    reg = collect_registry_goals(weekly_plans_col)\n",
    "\n",
    "    cursor = user_days_col.find({}, {\"user\": 1, \"sessions\": 1})\n",
    "    for d in cursor:\n",
    "        user = d.get(\"user\")\n",
    "        goals = reg.get(user, set())\n",
    "        sessions = d.get(\"sessions\", [])\n",
    "        for s in sessions if isinstance(sessions, list) else []:\n",
    "            gid = s.get(\"gid\") or s.get(\"linked_gid\")\n",
    "            if gid:\n",
    "                counts[\"sessions_with_gid\"] += 1\n",
    "                if gid not in goals:\n",
    "                    counts[\"missing_goal_refs\"] += 1\n",
    "    return counts\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    client = MongoClient(args.uri)\n",
    "    db = client[args.db]\n",
    "    user_days = db[\"user_days\"]\n",
    "    weekly_plans = db[\"weekly_plans\"]\n",
    "\n",
    "    print(\"üîé Index audit\")\n",
    "    ud_ix = ensure_expected_indexes(user_days, EXPECTED_INDEXES[\"user_days\"], create=False)\n",
    "    wp_ix = ensure_expected_indexes(weekly_plans, EXPECTED_INDEXES[\"weekly_plans\"], create=False)\n",
    "\n",
    "    print(f\"  user_days: present={ud_ix['present']}, missing={ud_ix['missing']}, stray={ud_ix['stray']}\")\n",
    "    print(f\"  weekly_plans: present={wp_ix['present']}, missing={wp_ix['missing']}, stray={wp_ix['stray']}\")\n",
    "\n",
    "    if args.drop_stray_indexes:\n",
    "        if ud_ix[\"stray\"]:\n",
    "            n = drop_stray_indexes(user_days, ud_ix[\"stray\"])\n",
    "            print(f\"  ‚úÖ Dropped {n} stray indexes from user_days\")\n",
    "        if wp_ix[\"stray\"]:\n",
    "            n = drop_stray_indexes(weekly_plans, wp_ix[\"stray\"])\n",
    "            print(f\"  ‚úÖ Dropped {n} stray indexes from weekly_plans\")\n",
    "\n",
    "    # Offer to create missing expected indexes (safe)\n",
    "    if ud_ix[\"missing\"] or wp_ix[\"missing\"]:\n",
    "        print(\"  ‚ÑπÔ∏è Creating any missing expected indexes‚Ä¶\")\n",
    "        ensure_expected_indexes(user_days, EXPECTED_INDEXES[\"user_days\"], create=True)\n",
    "        ensure_expected_indexes(weekly_plans, EXPECTED_INDEXES[\"weekly_plans\"], create=True)\n",
    "\n",
    "    print(\"\\nüß™ Document validation ‚Äî user_days\")\n",
    "    ud_counts = validate_user_days(user_days, fix=args.fix)\n",
    "    for k, v in ud_counts.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"\\nüß™ Document validation ‚Äî weekly_plans\")\n",
    "    wp_counts = validate_weekly_plans(weekly_plans, fix=args.fix)\n",
    "    for k, v in wp_counts.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"\\nüîó Referential integrity\")\n",
    "    ref_counts = validate_referential(user_days, weekly_plans)\n",
    "    for k, v in ref_counts.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"\\n‚ú® Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca458767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Ensuring indexes ==\n",
      "== Migrating goals from registry ==\n",
      "Goals upserted: 5\n",
      "== Migrating weekly plans to v2 ==\n",
      "Weekly plans migrated: 13\n",
      "== Migrating sessions from users_days ==\n",
      "Sessions inserted: 0; daily_rollups updated: 0\n",
      "Done. DRY_RUN = True\n"
     ]
    }
   ],
   "source": [
    "import os, re, pytz\n",
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\", \"mongodb+srv://prashanth01071995:pradsml%402025@cluster0.fsbic.mongodb.net/\")\n",
    "DB_NAME   = os.environ.get(\"DB_NAME\", \"time_tracker_db\")\n",
    "USER      = os.environ.get(\"USER_KEY\", \"prashanth\")\n",
    "IST       = pytz.timezone(\"Asia/Kolkata\")\n",
    "DRY_RUN   = True  # set False after verifying dry run output\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "\n",
    "def iso_week_key(dt_ist: datetime) -> str:\n",
    "    iso = dt_ist.isocalendar()\n",
    "    return f\"{iso.year}-{iso.week:02d}\"\n",
    "\n",
    "def parse_ist_dt(date_str: str, time_str: str | None):\n",
    "    # date: \"YYYY-MM-DD\"; time like \"09:34 PM\" or None\n",
    "    d = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    if time_str:\n",
    "        t = datetime.strptime(time_str.strip(), \"%I:%M %p\").time()\n",
    "        dt = IST.localize(datetime.combine(d, t))\n",
    "    else:\n",
    "        dt = IST.localize(datetime.combine(d, datetime.min.time()))\n",
    "    return dt\n",
    "\n",
    "def ensure_indexes():\n",
    "    if DRY_RUN: return\n",
    "    db.sessions.create_index([(\"user\",1), (\"week_key\",1), (\"t\",1)], name=\"user_week_t\")\n",
    "    db.sessions.create_index([(\"user\",1), (\"date\",1)], name=\"user_date\")\n",
    "    db.sessions.create_index([(\"goal_id\",1)], name=\"goal_lookup\", sparse=True)\n",
    "\n",
    "    db.weekly_plans_v2.create_index([(\"user\",1), (\"week_key\",1)], name=\"uniq_user_week\", unique=True)\n",
    "    db.weekly_plans_v2.create_index([(\"user\",1), (\"items.goal_id\",1)], name=\"items_goal\")\n",
    "\n",
    "    db.daily_rollups.create_index([(\"user\",1), (\"date\",1)], name=\"uniq_user_date\", unique=True)\n",
    "    db.daily_rollups.create_index([(\"user\",1), (\"week_key\",1)], name=\"user_week\")\n",
    "\n",
    "    db.goals.create_index([(\"user\",1), (\"status\",1), (\"category\",1)], name=\"user_status_cat\")\n",
    "    db.goals.create_index([(\"user\",1), (\"title\",1)], name=\"user_title\")\n",
    "\n",
    "def migrate_goals_from_registry():\n",
    "    reg = db.weekly_plans.find_one({\"_id\": f\"{USER}|registry\"})\n",
    "    if not reg:\n",
    "        print(\"No registry doc found; skipping goals migration.\")\n",
    "        return\n",
    "    goals = reg.get(\"goals\", {})\n",
    "    ops = []\n",
    "    for gid, g in goals.items():\n",
    "        doc = {\n",
    "            \"_id\": gid,  # preserve your string IDs\n",
    "            \"user\": USER,\n",
    "            \"title\": g.get(\"title\"),\n",
    "            \"category\": g.get(\"goal_type\") or g.get(\"category\") or \"Misc\",\n",
    "            \"status\": g.get(\"status\", \"In Progress\"),\n",
    "            \"created_at\": g.get(\"created_at\", datetime.utcnow()),\n",
    "            \"updated_at\": g.get(\"updated_at\", datetime.utcnow()),\n",
    "            # optional stats you already track\n",
    "            \"poms_completed\": g.get(\"poms_completed\", 0),\n",
    "            \"priority_band\": g.get(\"priority_band\"),\n",
    "        }\n",
    "        ops.append(UpdateOne({\"_id\": gid}, {\"$set\": doc}, upsert=True))\n",
    "    if ops and not DRY_RUN:\n",
    "        db.goals.bulk_write(ops)\n",
    "    print(f\"Goals upserted: {len(ops)}\")\n",
    "\n",
    "def migrate_weekly_plans():\n",
    "    cur = db.weekly_plans.find({\"type\": \"plan\", \"user\": USER})\n",
    "    count = 0\n",
    "    for p in cur:\n",
    "        week_start = p[\"week_start\"]  # \"YYYY-MM-DD\"\n",
    "        week_end   = p[\"week_end\"]\n",
    "        ws_dt = IST.localize(datetime.strptime(week_start, \"%Y-%m-%d\"))\n",
    "        wk = iso_week_key(ws_dt)\n",
    "        items = []\n",
    "        for row in p.get(\"goals_embedded\", []):\n",
    "            planned = int(row.get(\"planned\", 0))\n",
    "            cin = int(row.get(\"carryover_in\", 0))\n",
    "            items.append({\n",
    "                \"goal_id\": row[\"goal_id\"],\n",
    "                \"weight\": int(row.get(\"priority_weight\", 0)),\n",
    "                \"planned\": planned,\n",
    "                \"planned_plus_carry\": planned + cin\n",
    "            })\n",
    "        doc = {\n",
    "            \"_id\": f\"{USER}|{week_start}\",\n",
    "            \"user\": USER,\n",
    "            \"week_start\": week_start,\n",
    "            \"week_end\": week_end,\n",
    "            \"week_key\": wk,\n",
    "            \"capacity\": {\n",
    "                \"weekday\": int(p.get(\"capacity\", {}).get(\"weekday\", 0)),\n",
    "                \"weekend\": int(p.get(\"capacity\", {}).get(\"weekend\", 0)),\n",
    "                \"total\":   int(p.get(\"capacity\", {}).get(\"total\", p.get(\"total_poms\", 0))),\n",
    "            },\n",
    "            \"items\": items,\n",
    "            \"carryover\": [\n",
    "                {\"goal_id\": gid, \"count\": int(c)} for gid, c in {}\n",
    "            ],  # populate later if needed\n",
    "            \"created_at\": p.get(\"created_at\", datetime.utcnow()),\n",
    "            \"updated_at\": p.get(\"updated_at\", datetime.utcnow())\n",
    "        }\n",
    "        if not DRY_RUN:\n",
    "            db.weekly_plans_v2.replace_one({\"_id\": doc[\"_id\"]}, doc, upsert=True)\n",
    "        count += 1\n",
    "    print(f\"Weekly plans migrated: {count}\")\n",
    "\n",
    "def migrate_sessions_from_users_days():\n",
    "    cur = db.users_days.find({\"user\": USER})\n",
    "    inserted = 0\n",
    "    updated_rollups = 0\n",
    "    for d in cur:\n",
    "        date_str = d[\"date\"]                    # \"YYYY-MM-DD\"\n",
    "        date_dt  = IST.localize(datetime.strptime(date_str, \"%Y-%m-%d\"))\n",
    "        wk = iso_week_key(date_dt)\n",
    "        first_session_min = None\n",
    "        sess_arr = d.get(\"sessions\", [])\n",
    "        for s in sess_arr:\n",
    "            t = s.get(\"t\")  # \"W\" or \"B\"\n",
    "            dur = int(s.get(\"dur\", 0))\n",
    "            time_str = s.get(\"time\")\n",
    "            started = parse_ist_dt(date_str, time_str)\n",
    "            ended   = started + timedelta(minutes=dur)\n",
    "            if first_session_min is None and t == \"W\":\n",
    "                first_session_min = started.hour * 60 + started.minute\n",
    "\n",
    "            doc = {\n",
    "                \"user\": USER,\n",
    "                \"date\": date_str,\n",
    "                \"week_key\": wk,\n",
    "                \"t\": t,\n",
    "                \"dur_min\": dur,\n",
    "                \"started_at_ist\": started,\n",
    "                \"ended_at_ist\": ended,\n",
    "                \"deep_work\": (t == \"W\" and dur >= 23),\n",
    "                \"context_switch\": False,\n",
    "                \"goal_mode\": \"custom\",  # can be improved with title‚Üígoal map\n",
    "                \"goal_id\": None,\n",
    "                \"task\": s.get(\"task\"),\n",
    "                \"cat\": s.get(\"cat\", \"Misc\"),\n",
    "                \"break_autostart\": None,\n",
    "                \"skipped\": False if t == \"B\" else None,\n",
    "            }\n",
    "            if not DRY_RUN:\n",
    "                db.sessions.insert_one(doc)\n",
    "            inserted += 1\n",
    "\n",
    "        # augment daily doc with week_key & first_session_min\n",
    "        upd = {\n",
    "            \"$set\": {\n",
    "                \"week_key\": wk,\n",
    "                \"first_session_min\": first_session_min if first_session_min is not None else None\n",
    "            }\n",
    "        }\n",
    "        if not DRY_RUN:\n",
    "            db.users_days.update_one({\"_id\": d[\"_id\"]}, upd)\n",
    "        updated_rollups += 1\n",
    "    print(f\"Sessions inserted: {inserted}; daily_rollups updated: {updated_rollups}\")\n",
    "\n",
    "def main():\n",
    "    print(\"== Ensuring indexes ==\")\n",
    "    ensure_indexes()\n",
    "    print(\"== Migrating goals from registry ==\")\n",
    "    migrate_goals_from_registry()\n",
    "    print(\"== Migrating weekly plans to v2 ==\")\n",
    "    migrate_weekly_plans()\n",
    "    print(\"== Migrating sessions from users_days ==\")\n",
    "    migrate_sessions_from_users_days()\n",
    "    print(\"Done. DRY_RUN =\", DRY_RUN)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f03fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cfg] DB_NAME=time_tracker_db USER=prashanth DRY_RUN=True MONGO_URI=(set)\n",
      "Collections: ['weekly_plans', 'users', 'reflections', 'logs', 'user_days', 'goals']\n",
      "== Ensuring indexes ==\n",
      "[dry-run] Skipping index creation.\n",
      "== Migrating goals from registry ==\n",
      "Goals upserted: 5\n",
      "== Migrating weekly plans to v2 ==\n",
      "Weekly plans migrated: 13\n",
      "== Migrating sessions from users_days ==\n",
      "[info] using source collection: user_days\n",
      "[info] daily docs to process: 41\n",
      "Sessions upserted: 157; daily docs updated: 41\n",
      "== Finalize weekly plans rename (optional) ==\n",
      "[dry-run] Skipping rename weekly_plans_v2‚Üíweekly_plans.\n",
      "Done. DRY_RUN = True\n"
     ]
    }
   ],
   "source": [
    "# migrate_focus_timer.py\n",
    "# Unified migration to scalable schema with sessions SoT + fuzzy task‚Üígoal mapping.\n",
    "# Requirements: pymongo, pytz (standard for your project). No extra packages.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import difflib\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "\n",
    "import pytz\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from pymongo.collection import Collection\n",
    "\n",
    "# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\", \"mongodb+srv://prashanth01071995:pradsml%402025@cluster0.fsbic.mongodb.net/\")\n",
    "DB_NAME   = os.environ.get(\"DB_NAME\", \"time_tracker_db\")\n",
    "USER      = os.environ.get(\"USER_KEY\", \"prashanth\")\n",
    "\n",
    "# DRY_RUN accepts: \"true/1/yes\" ‚Üí True; anything else ‚Üí False\n",
    "DRY_RUN = os.environ.get(\"DRY_RUN\", \"true\").strip().lower() in {\"true\", \"1\", \"yes\", \"y\"}\n",
    "\n",
    "IST = pytz.timezone(\"Asia/Kolkata\")\n",
    "\n",
    "# ‚îÄ‚îÄ Connect ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "\n",
    "# ‚îÄ‚îÄ Utilities ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def iso_week_key(dt_ist: datetime) -> str:\n",
    "    \"\"\"ISO week key as 'YYYY-WW' using IST-localized datetime.\"\"\"\n",
    "    iso = dt_ist.isocalendar()\n",
    "    return f\"{iso.year}-{iso.week:02d}\"\n",
    "\n",
    "def parse_ist_dt(date_str: str, time_str: Optional[str]) -> datetime:\n",
    "    \"\"\"Build IST-aware datetime from 'YYYY-MM-DD' and a time like '09:34 PM' (or None).\"\"\"\n",
    "    d = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    if time_str:\n",
    "        t = datetime.strptime(time_str.strip(), \"%I:%M %p\").time()\n",
    "        dt = IST.localize(datetime.combine(d, t))\n",
    "    else:\n",
    "        dt = IST.localize(datetime.combine(d, datetime.min.time()))\n",
    "    return dt\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Normalize a label for fuzzy matching.\"\"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[\\-\\_\\/&]+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def make_session_id(user: str, date_str: str, started_at: datetime, t: str, dur_min: int) -> str:\n",
    "    \"\"\"Deterministic _id to keep migration idempotent on re-runs.\"\"\"\n",
    "    ts = int(started_at.timestamp() * 1000)  # ms\n",
    "    return f\"{user}|{date_str}|{t}|{ts}|{dur_min}\"\n",
    "\n",
    "# ‚îÄ‚îÄ Index setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def ensure_indexes():\n",
    "    if DRY_RUN:\n",
    "        print(\"[dry-run] Skipping index creation.\")\n",
    "        return\n",
    "\n",
    "    # Sessions (SoT)\n",
    "    db.sessions.create_index([(\"user\",1), (\"week_key\",1), (\"t\",1)], name=\"user_week_t\")\n",
    "    db.sessions.create_index([(\"user\",1), (\"date\",1)],                   name=\"user_date\")\n",
    "    db.sessions.create_index([(\"goal_id\",1)],                            name=\"goal_lookup\", sparse=True)\n",
    "\n",
    "    # Weekly plans v2 (staging)\n",
    "    db.weekly_plans_v2.create_index([(\"user\",1), (\"week_key\",1)],        name=\"uniq_user_week\", unique=True)\n",
    "    db.weekly_plans_v2.create_index([(\"user\",1), (\"items.goal_id\",1)],   name=\"items_goal\")\n",
    "\n",
    "    # Daily rollups\n",
    "    db.users_days.create_index([(\"user\",1), (\"date\",1)],                 name=\"uniq_user_date\", unique=True)\n",
    "    db.users_days.create_index([(\"user\",1), (\"week_key\",1)],             name=\"user_week\")\n",
    "\n",
    "    # Goals\n",
    "    db.goals.create_index([(\"user\",1), (\"status\",1), (\"category\",1)],    name=\"user_status_cat\")\n",
    "    db.goals.create_index([(\"user\",1), (\"title\",1)],                     name=\"user_title\")\n",
    "\n",
    "# ‚îÄ‚îÄ Source collection detection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def pick_users_days_collection() -> Collection:\n",
    "    names = set(db.list_collection_names())\n",
    "    for cand in [\"users_days\", \"user_days\", \"daily_rollups\"]:\n",
    "        if cand in names:\n",
    "            print(f\"[info] using source collection: {cand}\")\n",
    "            return db.get_collection(cand)\n",
    "    raise RuntimeError(\"No users_days-like collection found (looked for users_days/user_days/daily_rollups).\")\n",
    "\n",
    "# ‚îÄ‚îÄ Goal registry ‚Üí goals collection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def migrate_goals_from_registry() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns: dict goal_id -> title (for fuzzy matching later).\n",
    "    \"\"\"\n",
    "    reg = db.weekly_plans.find_one({\"_id\": f\"{USER}|registry\"})\n",
    "    if not reg:\n",
    "        print(\"[warn] No registry doc found; skipping goals migration.\")\n",
    "        return {}\n",
    "    goals = reg.get(\"goals\", {})\n",
    "    ops = []\n",
    "    title_map = {}\n",
    "    for gid, g in goals.items():\n",
    "        title = g.get(\"title\")\n",
    "        doc = {\n",
    "            \"_id\": gid,  # preserve your string IDs\n",
    "            \"user\": USER,\n",
    "            \"title\": title,\n",
    "            \"category\": g.get(\"goal_type\") or g.get(\"category\") or \"Misc\",\n",
    "            \"status\": g.get(\"status\", \"In Progress\"),\n",
    "            \"created_at\": g.get(\"created_at\", datetime.utcnow()),\n",
    "            \"updated_at\": g.get(\"updated_at\", datetime.utcnow()),\n",
    "            \"poms_completed\": g.get(\"poms_completed\", 0),\n",
    "            \"priority_band\": g.get(\"priority_band\"),\n",
    "        }\n",
    "        ops.append(UpdateOne({\"_id\": gid}, {\"$set\": doc}, upsert=True))\n",
    "        if title:\n",
    "            title_map[gid] = title\n",
    "    if ops and not DRY_RUN:\n",
    "        db.goals.bulk_write(ops)\n",
    "    print(f\"Goals upserted: {len(ops)}\")\n",
    "    return title_map\n",
    "\n",
    "# ‚îÄ‚îÄ Weekly plan migration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def migrate_weekly_plans() -> None:\n",
    "    cur = db.weekly_plans.find({\"type\": \"plan\", \"user\": USER})\n",
    "    count = 0\n",
    "    for p in cur:\n",
    "        week_start = p[\"week_start\"]  # \"YYYY-MM-DD\"\n",
    "        week_end   = p[\"week_end\"]\n",
    "        ws_dt = IST.localize(datetime.strptime(week_start, \"%Y-%m-%d\"))\n",
    "        wk = iso_week_key(ws_dt)\n",
    "\n",
    "        items = []\n",
    "        carry_list = []\n",
    "        for row in p.get(\"goals_embedded\", []):\n",
    "            planned = int(row.get(\"planned\", 0))\n",
    "            cin = int(row.get(\"carryover_in\", 0))\n",
    "            gid = row[\"goal_id\"]\n",
    "            items.append({\n",
    "                \"goal_id\": gid,\n",
    "                \"weight\": int(row.get(\"priority_weight\", 0)),\n",
    "                \"planned\": planned,\n",
    "                \"planned_plus_carry\": planned + cin\n",
    "            })\n",
    "            if cin > 0:\n",
    "                carry_list.append({\"goal_id\": gid, \"count\": cin})\n",
    "\n",
    "        # capacity block\n",
    "        cap_src = p.get(\"capacity\", {})\n",
    "        total_cap = int(cap_src.get(\"total\", p.get(\"total_poms\", 0)))\n",
    "        doc = {\n",
    "            \"_id\": f\"{USER}|{week_start}\",\n",
    "            \"user\": USER,\n",
    "            \"week_start\": week_start,\n",
    "            \"week_end\": week_end,\n",
    "            \"week_key\": wk,\n",
    "            \"capacity\": {\n",
    "                \"weekday\": int(cap_src.get(\"weekday\", 0)),\n",
    "                \"weekend\": int(cap_src.get(\"weekend\", 0)),\n",
    "                \"total\":   total_cap,\n",
    "            },\n",
    "            \"items\": items,\n",
    "            \"carryover\": carry_list,\n",
    "            \"created_at\": p.get(\"created_at\", datetime.utcnow()),\n",
    "            \"updated_at\": p.get(\"updated_at\", datetime.utcnow())\n",
    "        }\n",
    "\n",
    "        if not DRY_RUN:\n",
    "            db.weekly_plans_v2.replace_one({\"_id\": doc[\"_id\"]}, doc, upsert=True)\n",
    "        count += 1\n",
    "    print(f\"Weekly plans migrated: {count}\")\n",
    "\n",
    "# ‚îÄ‚îÄ Fuzzy task ‚Üí goal_id mapping ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "class GoalMatcher:\n",
    "    def __init__(self, goal_titles_by_id: Dict[str, str]):\n",
    "        # normalized title ‚Üí goal_id (allow collisions; we‚Äôll break ties later)\n",
    "        self.norm_to_ids: Dict[str, List[str]] = {}\n",
    "        self.id_to_norm: Dict[str, str] = {}\n",
    "        for gid, title in goal_titles_by_id.items():\n",
    "            norm_t = normalize(title)\n",
    "            self.id_to_norm[gid] = norm_t\n",
    "            self.norm_to_ids.setdefault(norm_t, []).append(gid)\n",
    "        self.all_norm_titles = list(self.norm_to_ids.keys())\n",
    "\n",
    "    def match(self, task: Optional[str]) -> Optional[str]:\n",
    "        if not task:\n",
    "            return None\n",
    "        ntask = normalize(task)\n",
    "        if not ntask:\n",
    "            return None\n",
    "\n",
    "        # 1) exact normalized title ‚Üí unique goal\n",
    "        if ntask in self.norm_to_ids:\n",
    "            ids = self.norm_to_ids[ntask]\n",
    "            return ids[0] if len(ids) == 1 else None\n",
    "\n",
    "        # 2) substring heuristic (task within title or title within task)\n",
    "        for norm_title, ids in self.norm_to_ids.items():\n",
    "            if ntask in norm_title or norm_title in ntask:\n",
    "                return ids[0] if len(ids) == 1 else None\n",
    "\n",
    "        # 3) fuzzy match (difflib)\n",
    "        best = None\n",
    "        best_score = 0.0\n",
    "        runner = 0.0\n",
    "        for norm_title in self.all_norm_titles:\n",
    "            score = difflib.SequenceMatcher(None, ntask, norm_title).ratio()\n",
    "            if score > best_score:\n",
    "                runner = best_score\n",
    "                best_score = score\n",
    "                best = norm_title\n",
    "            elif score > runner:\n",
    "                runner = score\n",
    "\n",
    "        # accept only if confidently above threshold and not ambiguous\n",
    "        if best and best_score >= 0.72 and (best_score - runner) >= 0.08:\n",
    "            ids = self.norm_to_ids[best]\n",
    "            return ids[0] if len(ids) == 1 else None\n",
    "        return None\n",
    "\n",
    "# ‚îÄ‚îÄ Cache of weekly plan goal IDs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "_plan_goal_cache: Dict[str, Set[str]] = {}\n",
    "\n",
    "def week_goal_ids(week_key: str) -> Set[str]:\n",
    "    if week_key in _plan_goal_cache:\n",
    "        return _plan_goal_cache[week_key]\n",
    "    row = db.weekly_plans_v2.find_one({\"user\": USER, \"week_key\": week_key}, {\"items.goal_id\": 1})\n",
    "    s: Set[str] = set()\n",
    "    if row and \"items\" in row:\n",
    "        for it in row[\"items\"]:\n",
    "            gid = it.get(\"goal_id\")\n",
    "            if gid:\n",
    "                s.add(gid)\n",
    "    _plan_goal_cache[week_key] = s\n",
    "    return s\n",
    "\n",
    "# ‚îÄ‚îÄ Sessions migration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def migrate_sessions_from_users_days(goal_matcher: GoalMatcher):\n",
    "    src = pick_users_days_collection()\n",
    "\n",
    "    # primary filter by `user`, fallback by `_id` prefix\n",
    "    q_user = {\"user\": USER}\n",
    "    n_user = src.count_documents(q_user)\n",
    "    used_id_prefix = False\n",
    "    if n_user == 0:\n",
    "        print(f\"[warn] No docs with user='{USER}'. Falling back to _id prefix.\")\n",
    "        q_user = {\"_id\": {\"$regex\": f\"^{USER}\\\\|\"}}\n",
    "        n_user = src.count_documents(q_user)\n",
    "        used_id_prefix = True\n",
    "    print(f\"[info] daily docs to process: {n_user}\")\n",
    "\n",
    "    cur = src.find(q_user)\n",
    "    inserted = 0\n",
    "    updated_rollups = 0\n",
    "    for d in cur:\n",
    "        # date detection\n",
    "        date_str = d.get(\"date\")\n",
    "        if not date_str:\n",
    "            _id = d.get(\"_id\")\n",
    "            if isinstance(_id, str) and \"|\" in _id:\n",
    "                date_str = _id.split(\"|\", 1)[1]\n",
    "            else:\n",
    "                # cannot infer; skip\n",
    "                continue\n",
    "\n",
    "        date_dt = IST.localize(datetime.strptime(date_str, \"%Y-%m-%d\"))\n",
    "        wk = iso_week_key(date_dt)\n",
    "        first_session_min = None\n",
    "\n",
    "        sess_arr = d.get(\"sessions\", []) or []\n",
    "        for s in sess_arr:\n",
    "            t = s.get(\"t\")  # \"W\" or \"B\"\n",
    "            dur = int(s.get(\"dur\", 0))\n",
    "            time_str = s.get(\"time\")  # e.g., \"09:34 PM\"\n",
    "            started = parse_ist_dt(date_str, time_str) if time_str else date_dt\n",
    "            ended   = started + timedelta(minutes=dur)\n",
    "\n",
    "            # Compute first session minutes (only for work)\n",
    "            if first_session_min is None and t == \"W\":\n",
    "                first_session_min = started.hour * 60 + started.minute\n",
    "\n",
    "            # Fuzzy match to goal\n",
    "            task_label = s.get(\"task\")\n",
    "            matched_goal = goal_matcher.match(task_label)\n",
    "\n",
    "            # Decide goal_mode\n",
    "            mode = \"custom\"\n",
    "            if matched_goal:\n",
    "                if matched_goal in week_goal_ids(wk):\n",
    "                    mode = \"weekly\"\n",
    "                else:\n",
    "                    # the goal exists, but wasn't in that week's plan ‚Üí still custom for analytics\n",
    "                    mode = \"custom\"\n",
    "\n",
    "            # Build doc\n",
    "            doc = {\n",
    "                \"_id\": make_session_id(USER, date_str, started, t or \"?\", dur),\n",
    "                \"user\": USER,\n",
    "                \"date\": date_str,\n",
    "                \"week_key\": wk,\n",
    "                \"t\": t,\n",
    "                \"dur_min\": dur,\n",
    "                \"started_at_ist\": started,\n",
    "                \"ended_at_ist\": ended,\n",
    "                \"deep_work\": (t == \"W\" and dur >= 23),\n",
    "                \"context_switch\": False,\n",
    "                \"goal_mode\": mode,\n",
    "                \"goal_id\": matched_goal,\n",
    "                \"task\": task_label,\n",
    "                \"cat\": s.get(\"cat\", \"Misc\"),\n",
    "                \"break_autostart\": None,\n",
    "                \"skipped\": (False if t == \"B\" else None),\n",
    "            }\n",
    "\n",
    "            if not DRY_RUN:\n",
    "                # idempotent replace keeps re-runs clean\n",
    "                db.sessions.replace_one({\"_id\": doc[\"_id\"]}, doc, upsert=True)\n",
    "            inserted += 1\n",
    "\n",
    "        # augment rollup doc in place: week_key + first_session_min\n",
    "        upd = {\"$set\": {\"week_key\": wk, \"first_session_min\": first_session_min}}\n",
    "        if not DRY_RUN:\n",
    "            src.update_one({\"_id\": d[\"_id\"]}, upd, upsert=False)\n",
    "        updated_rollups += 1\n",
    "\n",
    "    print(f\"Sessions upserted: {inserted}; daily docs updated: {updated_rollups}\")\n",
    "\n",
    "# ‚îÄ‚îÄ Optional: finalize rename ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def finalize_weekly_plans():\n",
    "    \"\"\"Rename weekly_plans_v2 ‚Üí weekly_plans (drop target if exists).\"\"\"\n",
    "    if DRY_RUN:\n",
    "        print(\"[dry-run] Skipping rename weekly_plans_v2‚Üíweekly_plans.\")\n",
    "        return\n",
    "    names = set(db.list_collection_names())\n",
    "    if \"weekly_plans_v2\" not in names:\n",
    "        print(\"[info] weekly_plans_v2 not found; nothing to rename.\")\n",
    "        return\n",
    "    # If weekly_plans already exists and you want to replace it, drop it first.\n",
    "    if \"weekly_plans\" in names:\n",
    "        db.weekly_plans.drop()\n",
    "    db.weekly_plans_v2.rename(\"weekly_plans\")\n",
    "    print(\"[ok] Renamed weekly_plans_v2 ‚Üí weekly_plans\")\n",
    "\n",
    "# ‚îÄ‚îÄ Entrypoint ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def main():\n",
    "    print(\"== Ensuring indexes ==\")\n",
    "    ensure_indexes()\n",
    "\n",
    "    print(\"== Migrating goals from registry ==\")\n",
    "    titles_by_id = migrate_goals_from_registry()\n",
    "    goal_matcher = GoalMatcher(titles_by_id)\n",
    "\n",
    "    print(\"== Migrating weekly plans to v2 ==\")\n",
    "    migrate_weekly_plans()\n",
    "\n",
    "    print(\"== Migrating sessions from users_days ==\")\n",
    "    migrate_sessions_from_users_days(goal_matcher)\n",
    "\n",
    "    print(\"== Finalize weekly plans rename (optional) ==\")\n",
    "    finalize_weekly_plans()\n",
    "\n",
    "    print(\"Done. DRY_RUN =\", DRY_RUN)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick visibility of environment\n",
    "    print(f\"[cfg] DB_NAME={DB_NAME} USER={USER} DRY_RUN={DRY_RUN} MONGO_URI={'(set)' if MONGO_URI else '(missing)'}\")\n",
    "    print(\"Collections:\", db.list_collection_names())\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6353a6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cfg] DB=time_tracker_db USER=prashanth MONGO_URI=(set)\n",
      "== Collections present ==\n",
      "goals, logs, reflections, user_days, users, weekly_plans\n",
      "\n",
      "== Global Counts ==\n",
      "Sessions total : 0  (W=0, B=0)\n",
      "Goals          : 5\n",
      "Weekly plans   : 14\n",
      "\n",
      "== Weeks Coverage ==\n",
      "Weeks in sessions: 0\n",
      "Weeks in plans   : 0\n",
      "OK: sessions & plans cover the same weeks.\n",
      "\n",
      "== Plan Goals vs Goals Registry ==\n",
      "OK: all plan goal_ids exist in goals collection.\n",
      "\n",
      "== Unmapped Work Sessions (goal_id is null) ==\n",
      "Total unmapped work sessions: 0\n",
      "OK: no unmapped work sessions.\n",
      "\n",
      "== Per-week KPIs ==\n",
      "No weeks found in sessions.\n",
      "\n",
      "== Day Rollup Coverage ==\n",
      "Session dates: 0, Rollup dates: 41\n",
      "Dates with rollup but NO sessions  : 2025-05-31, 2025-06-01, 2025-06-02, 2025-06-03, 2025-06-04, 2025-06-05, 2025-06-06, 2025-06-07, 2025-06-08, 2025-06-10, 2025-06-15, 2025-06-18, 2025-06-21, 2025-06-26, 2025-06-28, 2025-07-03, 2025-07-04, 2025-07-09, 2025-07-10, 2025-07-14, 2025-07-20, 2025-07-22, 2025-07-26, 2025-07-28, 2025-07-29 ...\n",
      "\n",
      "== Sample Week Detail ==\n",
      "No sessions found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "migration_sanity_checks.py\n",
    "Sanity checks for Focus Timer DB after migration.\n",
    "\n",
    "Usage (env vars):\n",
    "  export MONGO_URI=\"mongodb+srv://...\"\n",
    "  export DB_NAME=\"time_tracker_db\"\n",
    "  export USER_KEY=\"prashanth\"\n",
    "  python migration_sanity_checks.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pytz\n",
    "\n",
    "IST = pytz.timezone(\"Asia/Kolkata\")\n",
    "\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\", \"mongodb+srv://prashanth01071995:pradsml%402025@cluster0.fsbic.mongodb.net/\")\n",
    "DB_NAME   = os.environ.get(\"DB_NAME\", \"time_tracker_db\")\n",
    "USER      = os.environ.get(\"USER_KEY\", \"prashanth\")\n",
    "\n",
    "def iso_week_key(dt_ist: datetime) -> str:\n",
    "    iso = dt_ist.isocalendar()\n",
    "    return f\"{iso.year}-{iso.week:02d}\"\n",
    "\n",
    "def pick_rollup_collection(db) -> Optional[str]:\n",
    "    names = set(db.list_collection_names())\n",
    "    for cand in [\"users_days\", \"user_days\", \"daily_rollups\"]:\n",
    "        if cand in names:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "def num(val) -> str:\n",
    "    return f\"{val:,}\"\n",
    "\n",
    "def pct(n, d) -> str:\n",
    "    if d == 0:\n",
    "        return \"‚Äî\"\n",
    "    return f\"{(n/d)*100:5.1f}%\"\n",
    "\n",
    "def safe_sum(a):\n",
    "    return sum(x for x in a if isinstance(x, (int, float)))\n",
    "\n",
    "def get_all_week_keys(db, user: str) -> List[str]:\n",
    "    return sorted(list(db.sessions.distinct(\"week_key\", {\"user\": user})))\n",
    "\n",
    "def check_collections(db):\n",
    "    print(\"== Collections present ==\")\n",
    "    print(\", \".join(sorted(db.list_collection_names())))\n",
    "    print()\n",
    "\n",
    "def check_counts(db, user: str):\n",
    "    print(\"== Global Counts ==\")\n",
    "    ses_total = db.sessions.count_documents({\"user\": user})\n",
    "    ses_work  = db.sessions.count_documents({\"user\": user, \"t\": \"W\"})\n",
    "    ses_break = db.sessions.count_documents({\"user\": user, \"t\": \"B\"})\n",
    "    goals     = db.goals.count_documents({\"user\": user})\n",
    "    plans_v2  = db.weekly_plans.count_documents({\"user\": user})\n",
    "    print(f\"Sessions total : {num(ses_total)}  (W={num(ses_work)}, B={num(ses_break)})\")\n",
    "    print(f\"Goals          : {num(goals)}\")\n",
    "    print(f\"Weekly plans   : {num(plans_v2)}\")\n",
    "    print()\n",
    "\n",
    "def check_weeks_coverage(db, user: str):\n",
    "    print(\"== Weeks Coverage ==\")\n",
    "    ses_weeks  = set(get_all_week_keys(db, user))\n",
    "    plan_weeks = set(db.weekly_plans.distinct(\"week_key\", {\"user\": user}))\n",
    "    only_ses   = sorted(ses_weeks - plan_weeks)\n",
    "    only_plan  = sorted(plan_weeks - ses_weeks)\n",
    "    print(f\"Weeks in sessions: {len(ses_weeks)}\")\n",
    "    print(f\"Weeks in plans   : {len(plan_weeks)}\")\n",
    "    if only_ses:\n",
    "        print(\"Weeks with sessions but NO plan:\", \", \".join(only_ses))\n",
    "    if only_plan:\n",
    "        print(\"Weeks with plan but NO sessions:\", \", \".join(only_plan))\n",
    "    if not only_ses and not only_plan:\n",
    "        print(\"OK: sessions & plans cover the same weeks.\")\n",
    "    print()\n",
    "\n",
    "def check_unknown_plan_goals(db, user: str):\n",
    "    print(\"== Plan Goals vs Goals Registry ==\")\n",
    "    plan_goal_ids = set()\n",
    "    for row in db.weekly_plans.find({\"user\": user}, {\"items.goal_id\": 1}):\n",
    "        for it in row.get(\"items\", []):\n",
    "            gid = it.get(\"goal_id\")\n",
    "            if gid:\n",
    "                plan_goal_ids.add(gid)\n",
    "    reg_goal_ids = set(db.goals.distinct(\"_id\", {\"user\": user}))\n",
    "    unknown = sorted(list(plan_goal_ids - reg_goal_ids))\n",
    "    if unknown:\n",
    "        print(\"Unknown goal_ids referenced in plans (not in goals collection):\")\n",
    "        for gid in unknown:\n",
    "            print(\" -\", gid)\n",
    "    else:\n",
    "        print(\"OK: all plan goal_ids exist in goals collection.\")\n",
    "    print()\n",
    "\n",
    "def check_unmapped_tasks(db, user: str, top_n: int = 20):\n",
    "    print(\"== Unmapped Work Sessions (goal_id is null) ==\")\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"user\": user, \"t\": \"W\", \"goal_id\": None}},\n",
    "        {\"$group\": {\"_id\": \"$task\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": top_n},\n",
    "    ]\n",
    "    rows = list(db.sessions.aggregate(pipeline))\n",
    "    total_unmapped = db.sessions.count_documents({\"user\": user, \"t\": \"W\", \"goal_id\": None})\n",
    "    print(f\"Total unmapped work sessions: {num(total_unmapped)}\")\n",
    "    if rows:\n",
    "        for r in rows:\n",
    "            print(f\" - {r['_id']!r}: {r['count']}\")\n",
    "    else:\n",
    "        print(\"OK: no unmapped work sessions.\")\n",
    "    print()\n",
    "\n",
    "def per_week_kpis(db, user: str):\n",
    "    print(\"== Per-week KPIs ==\")\n",
    "    weeks = get_all_week_keys(db, user)\n",
    "    if not weeks:\n",
    "        print(\"No weeks found in sessions.\")\n",
    "        print()\n",
    "        return\n",
    "\n",
    "    header = f\"{'Week':<8} {'Planned':>7} {'Actual':>7} {'Adh%':>6} | {'Deep%':>6} {'Break%':>7} | {'Wkly%':>6} {'Cust%':>6} {'Unmap':>6}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for wk in weeks:\n",
    "        # planned\n",
    "        planned = 0\n",
    "        plan_doc = db.weekly_plans.find_one({\"user\": user, \"week_key\": wk}, {\"items\": 1})\n",
    "        if plan_doc:\n",
    "            planned = sum(int(it.get(\"planned_plus_carry\", it.get(\"planned\", 0))) for it in plan_doc.get(\"items\", []))\n",
    "\n",
    "        # actual work count\n",
    "        actual = db.sessions.count_documents({\"user\": user, \"week_key\": wk, \"t\": \"W\"})\n",
    "\n",
    "        # deep work\n",
    "        deep = db.sessions.count_documents({\"user\": user, \"week_key\": wk, \"t\": \"W\", \"deep_work\": True})\n",
    "        deep_pct = (deep / actual * 100) if actual else 0.0\n",
    "\n",
    "        # break compliance: taken (>=4 min, not skipped) vs required (work sessions)\n",
    "        taken = db.sessions.count_documents({\n",
    "            \"user\": user, \"week_key\": wk, \"t\": \"B\",\n",
    "            \"dur_min\": {\"$gte\": 4},\n",
    "            \"$or\": [{\"skipped\": {\"$exists\": False}}, {\"skipped\": {\"$ne\": True}}]\n",
    "        })\n",
    "        req = actual\n",
    "        break_pct = (min(taken, req) / req * 100) if req else 0.0\n",
    "\n",
    "        # goal_mode split\n",
    "        wkly = db.sessions.count_documents({\"user\": user, \"week_key\": wk, \"t\": \"W\", \"goal_mode\": \"weekly\"})\n",
    "        cust = db.sessions.count_documents({\"user\": user, \"week_key\": wk, \"t\": \"W\", \"goal_mode\": \"custom\"})\n",
    "        wkly_pct = (wkly / actual * 100) if actual else 0.0\n",
    "        cust_pct = (cust / actual * 100) if actual else 0.0\n",
    "\n",
    "        # unmapped\n",
    "        unmapped = db.sessions.count_documents({\"user\": user, \"week_key\": wk, \"t\": \"W\", \"goal_id\": None})\n",
    "\n",
    "        # adherence\n",
    "        adh_pct = (min(actual / planned, 1) * 100) if planned else 0.0\n",
    "\n",
    "        print(f\"{wk:<8} {planned:7d} {actual:7d} {adh_pct:6.1f} | {deep_pct:6.1f} {break_pct:7.1f} | {wkly_pct:6.1f} {cust_pct:6.1f} {unmapped:6d}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "def day_rollup_gaps(db, user: str):\n",
    "    rollup_name = pick_rollup_collection(db)\n",
    "    print(\"== Day Rollup Coverage ==\")\n",
    "    if not rollup_name:\n",
    "        print(\"No users_days/user_days/daily_rollups collection found. Skipping.\")\n",
    "        print()\n",
    "        return\n",
    "    # Dates from sessions\n",
    "    s_dates = set(db.sessions.distinct(\"date\", {\"user\": user}))\n",
    "    # Dates from rollup\n",
    "    r_dates = set(db.get_collection(rollup_name).distinct(\"date\", {\"user\": user}))\n",
    "    missing = sorted(list(s_dates - r_dates))\n",
    "    extra   = sorted(list(r_dates - s_dates))\n",
    "    print(f\"Session dates: {len(s_dates)}, Rollup dates: {len(r_dates)}\")\n",
    "    if missing:\n",
    "        print(\"Dates with sessions but NO rollup doc:\", \", \".join(missing[:25]) + (\" ...\" if len(missing)>25 else \"\"))\n",
    "    if extra:\n",
    "        print(\"Dates with rollup but NO sessions  :\", \", \".join(extra[:25]) + (\" ...\" if len(extra)>25 else \"\"))\n",
    "    if not missing and not extra:\n",
    "        print(\"OK: rollups match session dates.\")\n",
    "    print()\n",
    "\n",
    "def print_sample_week(db, user: str, limit: int = 1):\n",
    "    print(\"== Sample Week Detail ==\")\n",
    "    wk = db.sessions.find_one({\"user\": user}, sort=[(\"week_key\", 1)])\n",
    "    if not wk:\n",
    "        print(\"No sessions found.\")\n",
    "        print()\n",
    "        return\n",
    "    wk_key = wk.get(\"week_key\")\n",
    "    if not wk_key:\n",
    "        print(\"No week_key on sample session.\")\n",
    "        print()\n",
    "        return\n",
    "    print(f\"Week: {wk_key}\")\n",
    "    # Print planned items\n",
    "    plan = db.weekly_plans.find_one({\"user\": user, \"week_key\": wk_key})\n",
    "    if plan:\n",
    "        items = plan.get(\"items\", [])\n",
    "        print(\"Planned items:\")\n",
    "        for it in items:\n",
    "            print(f\" - {it.get('goal_id')} | weight={it.get('weight')} planned={it.get('planned')} carry={it.get('planned_plus_carry', it.get('planned'))-it.get('planned',0)}\")\n",
    "    else:\n",
    "        print(\"No plan for this week.\")\n",
    "    # Print a few sessions\n",
    "    print(\"First 10 sessions:\")\n",
    "    for s in db.sessions.find({\"user\": user, \"week_key\": wk_key}).sort(\"started_at_ist\", 1).limit(10):\n",
    "        print(f\" {s.get('t')} {s.get('started_at_ist')} {s.get('dur_min')}m goal={s.get('goal_id')} mode={s.get('goal_mode')} task={s.get('task')}\")\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    print(f\"[cfg] DB={DB_NAME} USER={USER} MONGO_URI={'(set)' if MONGO_URI else '(missing)'}\")\n",
    "    cli = MongoClient(MONGO_URI)\n",
    "    db = cli[DB_NAME]\n",
    "\n",
    "    check_collections(db)\n",
    "    check_counts(db, USER)\n",
    "    check_weeks_coverage(db, USER)\n",
    "    check_unknown_plan_goals(db, USER)\n",
    "    check_unmapped_tasks(db, USER, top_n=20)\n",
    "    per_week_kpis(db, USER)\n",
    "    day_rollup_gaps(db, USER)\n",
    "    print_sample_week(db, USER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd81b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc96ca31",
   "metadata": {},
   "source": [
    "# v2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b13b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cfg] DB=Focus_DB INCLUDE_DAILY_RHYTHM=False\n",
      "[=] collection exists: users\n",
      "[=] collection exists: goals\n",
      "[=] collection exists: weekly_plans\n",
      "[=] collection exists: sessions\n",
      "[=] collection exists: reflections\n",
      "[=] collection exists: daily_targets\n",
      "[ok] applied validator: users\n",
      "[ok] applied validator: goals\n",
      "[ok] applied validator: weekly_plans\n",
      "[ok] applied validator: sessions\n",
      "[ok] applied validator: reflections\n",
      "[ok] applied validator: daily_targets\n",
      "[ok] indexes ensured\n",
      "[done] Collections created, validators applied, indexes ensured.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Creates MongoDB collections FIRST, then applies validators (collMod).\n",
    "Also ensures indexes. Idempotent.\n",
    "\n",
    "Env:\n",
    "  MONGO_URI              (required)\n",
    "  DB_NAME                (default: focus_timer_prod)\n",
    "  INCLUDE_DAILY_RHYTHM   (\"true\"/\"false\"; default: false)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import CollectionInvalid, OperationFailure\n",
    "\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\", \"mongodb+srv://prashanth01071995:pradsml%402025@cluster0.fsbic.mongodb.net/\")\n",
    "DB_NAME = os.environ.get(\"DB_NAME\", \"Focus_DB\")\n",
    "INCLUDE_DAILY_RHYTHM = os.environ.get(\"INCLUDE_DAILY_RHYTHM\", \"false\").strip().lower() in {\"1\",\"true\",\"yes\",\"y\"}\n",
    "\n",
    "if not MONGO_URI:\n",
    "    raise SystemExit(\"Please set MONGO_URI\")\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "\n",
    "def create_if_missing(name: str):\n",
    "    existing = set(db.list_collection_names())\n",
    "    if name in existing:\n",
    "        print(f\"[=] collection exists: {name}\")\n",
    "        return\n",
    "    try:\n",
    "        db.create_collection(name)\n",
    "        print(f\"[ok] created collection: {name}\")\n",
    "    except CollectionInvalid:\n",
    "        print(f\"[=] race/exists: {name}\")\n",
    "\n",
    "def apply_validator(name: str, validator: dict):\n",
    "    try:\n",
    "        db.command({\n",
    "            \"collMod\": name,\n",
    "            \"validator\": validator,\n",
    "            \"validationLevel\": \"moderate\"  # reject invalid docs, allow older ones to pass reads\n",
    "        })\n",
    "        print(f\"[ok] applied validator: {name}\")\n",
    "    except OperationFailure as e:\n",
    "        print(f\"[warn] collMod failed on {name}: {e.details.get('errmsg', str(e))}\")\n",
    "\n",
    "def ensure_indexes():\n",
    "    # USERS\n",
    "    db.users.create_index([(\"email\", 1)], name=\"uniq_email\", unique=True)\n",
    "\n",
    "    # GOALS\n",
    "    db.goals.create_index([(\"user\",1), (\"status\",1), (\"category\",1)], name=\"user_status_cat\")\n",
    "    db.goals.create_index([(\"user\",1), (\"title\",1)], name=\"user_title\")\n",
    "\n",
    "    # WEEKLY_PLANS\n",
    "    db.weekly_plans.create_index([(\"user\",1), (\"week_key\",1)], name=\"uniq_user_week\", unique=True)\n",
    "    db.weekly_plans.create_index([(\"user\",1), (\"items.goal_id\",1)], name=\"items_goal\")\n",
    "\n",
    "    # SESSIONS\n",
    "    db.sessions.create_index([(\"user\",1), (\"week_key\",1), (\"t\",1)], name=\"user_week_t\")\n",
    "    db.sessions.create_index([(\"user\",1), (\"date_ist\",1)], name=\"user_date\")\n",
    "    db.sessions.create_index([(\"user\",1), (\"week_key\",1), (\"goal_id\",1), (\"alloc_bucket\",1)], name=\"bucket_progress\")\n",
    "    db.sessions.create_index([(\"goal_id\",1)], name=\"goal_lookup\", sparse=True)\n",
    "\n",
    "    # REFLECTIONS\n",
    "    db.reflections.create_index([(\"user\",1), (\"date_ist\",1)], name=\"uniq_user_date\", unique=True)\n",
    "\n",
    "    # DAILY_TARGETS\n",
    "    db.daily_targets.create_index([(\"user\",1), (\"date_ist\",1)], name=\"uniq_user_date\", unique=True)\n",
    "\n",
    "    # DAILY_RHYTHM (optional)\n",
    "    if INCLUDE_DAILY_RHYTHM:\n",
    "        db.daily_rhythm.create_index([(\"user\",1), (\"date_ist\",1)], name=\"uniq_user_date\", unique=True)\n",
    "\n",
    "    print(\"[ok] indexes ensured\")\n",
    "\n",
    "# --------------------------\n",
    "# VALIDATORS (concise v1)\n",
    "# --------------------------\n",
    "users_validator = {\n",
    "  \"$jsonSchema\": {\n",
    "    \"bsonType\": \"object\",\n",
    "    \"required\": [\"_id\",\"email\",\"tz\",\"prefs\",\"created_at\",\"updated_at\",\"schema_version\"],\n",
    "    \"properties\": {\n",
    "      \"_id\": {\"bsonType\":\"string\"},\n",
    "      \"email\": {\"bsonType\":\"string\",\"pattern\": \".+@.+\"},\n",
    "      \"tz\": {\"enum\": [\"Asia/Kolkata\"]},\n",
    "      \"prefs\": {\n",
    "        \"bsonType\":\"object\",\n",
    "        \"required\": [\"sound\",\"auto_break\",\"preferred_start_window\",\"rank_weight_map\",\"balance_band\",\"pom_default\"],\n",
    "        \"properties\": {\n",
    "          \"sound\": {\"bsonType\":\"bool\"},\n",
    "          \"auto_break\": {\"bsonType\":\"bool\"},\n",
    "          \"preferred_start_window\": {\n",
    "            \"bsonType\":\"object\",\n",
    "            \"required\":[\"start\",\"end\"],\n",
    "            \"properties\":{\n",
    "              \"start\":{\"bsonType\":\"string\",\"pattern\":\"^[0-2][0-9]:[0-5][0-9]$\"},\n",
    "              \"end\":{\"bsonType\":\"string\",\"pattern\":\"^[0-2][0-9]:[0-5][0-9]$\"}\n",
    "            }\n",
    "          },\n",
    "          \"rank_weight_map\": {\"bsonType\":\"object\",\"additionalProperties\":{\"bsonType\":\"int\",\"minimum\":0}},\n",
    "          \"balance_band\": {\n",
    "            \"bsonType\":\"object\",\n",
    "            \"required\":[\"career_target_pct\",\"tolerance_pct\"],\n",
    "            \"properties\":{\n",
    "              \"career_target_pct\":{\"bsonType\":\"int\",\"minimum\":0,\"maximum\":100},\n",
    "              \"tolerance_pct\":{\"bsonType\":\"int\",\"minimum\":0,\"maximum\":50}\n",
    "            }\n",
    "          },\n",
    "          \"pom_default\": {\n",
    "            \"bsonType\":\"object\",\n",
    "            \"required\":[\"work_min\",\"break_min\"],\n",
    "            \"properties\":{\n",
    "              \"work_min\":{\"bsonType\":\"int\",\"minimum\":5},\n",
    "              \"break_min\":{\"bsonType\":\"int\",\"minimum\":1}\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"created_at\":{\"bsonType\":\"date\"},\n",
    "      \"updated_at\":{\"bsonType\":\"date\"},\n",
    "      \"schema_version\":{\"bsonType\":\"int\",\"minimum\":1}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "goals_validator = {\n",
    "  \"$jsonSchema\": {\n",
    "    \"bsonType\":\"object\",\n",
    "    \"required\":[\"_id\",\"user\",\"title\",\"category\",\"status\",\"created_at\",\"updated_at\",\"schema_version\"],\n",
    "    \"properties\":{\n",
    "      \"_id\":{\"bsonType\":\"string\"},\n",
    "      \"user\":{\"bsonType\":\"string\"},\n",
    "      \"title\":{\"bsonType\":\"string\",\"minLength\":1},\n",
    "      \"category\":{\"enum\":[\"Learning\",\"Projects\",\"Certification\",\"Career\",\"Health\",\"Wellbeing\",\"Other\"]},\n",
    "      \"status\":{\"enum\":[\"In Progress\",\"On Hold\",\"Completed\"]},\n",
    "      \"tags\":{\"bsonType\":\"array\",\"items\":{\"bsonType\":\"string\"}},\n",
    "      \"is_primary\":{\"bsonType\":\"bool\"},\n",
    "      \"target_poms\":{\"bsonType\":[\"int\",\"null\"],\"minimum\":0},\n",
    "      \"created_at\":{\"bsonType\":\"date\"},\n",
    "      \"updated_at\":{\"bsonType\":\"date\"},\n",
    "      \"schema_version\":{\"bsonType\":\"int\",\"minimum\":1}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "weekly_plans_validator = {\n",
    "  \"$jsonSchema\":{\n",
    "    \"bsonType\":\"object\",\n",
    "    \"required\":[\"_id\",\"user\",\"week_key\",\"week_start\",\"week_end\",\"capacity\",\"items\",\"created_at\",\"updated_at\",\"schema_version\"],\n",
    "    \"properties\":{\n",
    "      \"_id\":{\"bsonType\":\"string\"},\n",
    "      \"user\":{\"bsonType\":\"string\"},\n",
    "      \"week_key\":{\"bsonType\":\"string\",\"pattern\":\"^[0-9]{4}-[0-9]{2}$\"},\n",
    "      \"week_start\":{\"bsonType\":\"string\",\"pattern\":\"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"},\n",
    "      \"week_end\":{\"bsonType\":\"string\",\"pattern\":\"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"},\n",
    "      \"capacity\":{\n",
    "        \"bsonType\":\"object\",\n",
    "        \"required\":[\"weekday\",\"weekend\",\"total\"],\n",
    "        \"properties\":{\n",
    "          \"weekday\":{\"bsonType\":\"int\",\"minimum\":0},\n",
    "          \"weekend\":{\"bsonType\":\"int\",\"minimum\":0},\n",
    "          \"total\":{\"bsonType\":\"int\",\"minimum\":0}\n",
    "        }\n",
    "      },\n",
    "      \"items\":{\n",
    "        \"bsonType\":\"array\",\"minItems\":1,\n",
    "        \"items\":{\n",
    "          \"bsonType\":\"object\",\n",
    "          \"required\":[\"goal_id\",\"priority_rank\",\"weight\",\"planned_current\",\"backlog_in\",\"total_target\"],\n",
    "          \"properties\":{\n",
    "            \"goal_id\":{\"bsonType\":\"string\"},\n",
    "            \"priority_rank\":{\"bsonType\":\"int\",\"minimum\":1},\n",
    "            \"weight\":{\"bsonType\":\"int\",\"minimum\":0,\"maximum\":10},\n",
    "            \"planned_current\":{\"bsonType\":\"int\",\"minimum\":0},\n",
    "            \"backlog_in\":{\"bsonType\":\"int\",\"minimum\":0},\n",
    "            \"total_target\":{\"bsonType\":\"int\",\"minimum\":0},\n",
    "            \"status_at_plan\":{\"enum\":[\"In Progress\",\"On Hold\",\"Completed\", None]},\n",
    "            \"close_action\":{\"enum\":[\"Completed\",\"On Hold\",\"Continue\", None]},\n",
    "            \"notes\":{\"bsonType\":[\"string\",\"null\"]}\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"created_at\":{\"bsonType\":\"date\"},\n",
    "      \"updated_at\":{\"bsonType\":\"date\"},\n",
    "      \"schema_version\":{\"bsonType\":\"int\",\"minimum\":1}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "sessions_validator = {\n",
    "  \"$jsonSchema\":{\n",
    "    \"bsonType\":\"object\",\n",
    "    \"required\":[\"_id\",\"user\",\"date_ist\",\"week_key\",\"t\",\"dur_min\",\"started_at_ist\",\"ended_at_ist\",\"schema_version\"],\n",
    "    \"properties\":{\n",
    "      \"_id\":{\"bsonType\":\"string\"},\n",
    "      \"user\":{\"bsonType\":\"string\"},\n",
    "      \"date_ist\":{\"bsonType\":\"string\",\"pattern\":\"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"},\n",
    "      \"week_key\":{\"bsonType\":\"string\",\"pattern\":\"^[0-9]{4}-[0-9]{2}$\"},\n",
    "      \"t\":{\"enum\":[\"W\",\"B\"]},\n",
    "      \"kind\":{\"enum\":[\"focus\",\"activity\", None]},\n",
    "      \"activity_type\":{\"enum\":[\"exercise\",\"meditation\",\"breathing\",\"other\", None]},\n",
    "      \"intensity\":{\"enum\":[\"light\",\"moderate\",\"vigorous\", None]},\n",
    "      \"dur_min\":{\"bsonType\":\"int\",\"minimum\":0},\n",
    "      \"pom_equiv\":{\"bsonType\":[\"double\",\"int\"],\"minimum\":0},\n",
    "      \"started_at_ist\":{\"bsonType\":\"date\"},\n",
    "      \"ended_at_ist\":{\"bsonType\":\"date\"},\n",
    "      \"deep_work\":{\"bsonType\":[\"bool\",\"null\"]},\n",
    "      \"context_switch\":{\"bsonType\":[\"bool\",\"null\"]},\n",
    "      \"goal_mode\":{\"enum\":[\"weekly\",\"custom\", None]},\n",
    "      \"goal_id\":{\"bsonType\":[\"string\",\"null\"]},\n",
    "      \"task\":{\"bsonType\":[\"string\",\"null\"]},\n",
    "      \"cat\":{\"bsonType\":[\"string\",\"null\"]},\n",
    "      \"alloc_bucket\":{\"enum\":[\"current\",\"backlog\", None]},\n",
    "      \"break_autostart\":{\"bsonType\":[\"bool\",\"null\"]},\n",
    "      \"skipped\":{\"bsonType\":[\"bool\",\"null\"]},\n",
    "      \"post_checkin\":{\n",
    "        \"bsonType\":[\"object\",\"null\"],\n",
    "        \"properties\":{\n",
    "          \"quality_1to5\":{\"bsonType\":[\"int\",\"null\"],\"minimum\":1,\"maximum\":5},\n",
    "          \"mood_1to5\":{\"bsonType\":[\"int\",\"null\"],\"minimum\":1,\"maximum\":5},\n",
    "          \"energy_1to5\":{\"bsonType\":[\"int\",\"null\"],\"minimum\":1,\"maximum\":5},\n",
    "          \"distraction\":{\"bsonType\":[\"string\",\"null\"]},\n",
    "          \"note\":{\"bsonType\":[\"string\",\"null\"]}\n",
    "        }\n",
    "      },\n",
    "      \"device\":{\"bsonType\":[\"string\",\"null\"]},\n",
    "      \"created_at\":{\"bsonType\":[\"date\",\"null\"]},\n",
    "      \"updated_at\":{\"bsonType\":[\"date\",\"null\"]},\n",
    "      \"schema_version\":{\"bsonType\":\"int\",\"minimum\":1}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "reflections_validator = {\n",
    "  \"$jsonSchema\":{\n",
    "    \"bsonType\":\"object\",\n",
    "    \"required\":[\"_id\",\"user\",\"date_ist\",\"reflection_submitted\",\"created_at\",\"updated_at\",\"schema_version\"],\n",
    "    \"properties\":{\n",
    "      \"_id\":{\"bsonType\":\"string\"},\n",
    "      \"user\":{\"bsonType\":\"string\"},\n",
    "      \"date_ist\":{\"bsonType\":\"string\",\"pattern\":\"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"},\n",
    "      \"focus_quality_1to5\":{\"bsonType\":[\"int\",\"null\"],\"minimum\":1,\"maximum\":5},\n",
    "      \"alignment_1to5\":{\"bsonType\":[\"int\",\"null\"],\"minimum\":1,\"maximum\":5},\n",
    "      \"blockers\":{\"bsonType\":[\"string\",\"null\"]},\n",
    "      \"insights\":{\"bsonType\":[\"string\",\"null\"]},\n",
    "      \"gratitude\":{\"bsonType\":[\"string\",\"null\"]},\n",
    "      \"notes\":{\n",
    "        \"bsonType\":[\"array\",\"null\"],\n",
    "        \"items\":{\n",
    "          \"bsonType\":\"object\",\n",
    "          \"required\":[\"at\",\"text\"],\n",
    "          \"properties\":{\n",
    "            \"at\":{\"bsonType\":\"date\"},\n",
    "            \"text\":{\"bsonType\":\"string\"},\n",
    "            \"goal_id\":{\"bsonType\":[\"string\",\"null\"]}\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"reflection_submitted\":{\"bsonType\":\"bool\"},\n",
    "      \"created_at\":{\"bsonType\":\"date\"},\n",
    "      \"updated_at\":{\"bsonType\":\"date\"},\n",
    "      \"schema_version\":{\"bsonType\":\"int\",\"minimum\":1}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "daily_targets_validator = {\n",
    "  \"$jsonSchema\":{\n",
    "    \"bsonType\":\"object\",\n",
    "    \"required\":[\"_id\",\"user\",\"date_ist\",\"target_pomos\",\"source\",\"created_at\",\"updated_at\",\"schema_version\"],\n",
    "    \"properties\":{\n",
    "      \"_id\":{\"bsonType\":\"string\"},\n",
    "      \"user\":{\"bsonType\":\"string\"},\n",
    "      \"date_ist\":{\"bsonType\":\"string\",\"pattern\":\"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"},\n",
    "      \"target_pomos\":{\"bsonType\":\"int\",\"minimum\":0},\n",
    "      \"target_minutes\":{\"bsonType\":[\"int\",\"null\"],\"minimum\":0},\n",
    "      \"source\":{\"enum\":[\"user\",\"derived_from_weekly_plan\"]},\n",
    "      \"created_at\":{\"bsonType\":\"date\"},\n",
    "      \"updated_at\":{\"bsonType\":\"date\"},\n",
    "      \"schema_version\":{\"bsonType\":\"int\",\"minimum\":1}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "daily_rhythm_validator = {\n",
    "  \"$jsonSchema\":{\n",
    "    \"bsonType\":\"object\",\n",
    "    \"required\":[\"_id\",\"user\",\"date_ist\",\"created_at\",\"updated_at\",\"schema_version\"],\n",
    "    \"properties\":{\n",
    "      \"_id\":{\"bsonType\":\"string\"},\n",
    "      \"user\":{\"bsonType\":\"string\"},\n",
    "      \"date_ist\":{\"bsonType\":\"string\",\"pattern\":\"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"},\n",
    "      \"sleep_start_ist\":{\"bsonType\":[\"date\",\"null\"]},\n",
    "      \"wake_time_ist\":{\"bsonType\":[\"date\",\"null\"]},\n",
    "      \"sleep_minutes\":{\"bsonType\":[\"int\",\"null\"],\"minimum\":0},\n",
    "      \"sleep_quality_1to5\":{\"bsonType\":[\"int\",\"null\"],\"minimum\":1,\"maximum\":5},\n",
    "      \"naps\":{\n",
    "        \"bsonType\":[\"array\",\"null\"],\n",
    "        \"items\":{\"bsonType\":\"object\",\"required\":[\"start\",\"minutes\"],\"properties\":{\n",
    "          \"start\":{\"bsonType\":\"date\"},\n",
    "          \"minutes\":{\"bsonType\":\"int\",\"minimum\":1}\n",
    "        }}\n",
    "      },\n",
    "      \"created_at\":{\"bsonType\":\"date\"},\n",
    "      \"updated_at\":{\"bsonType\":\"date\"},\n",
    "      \"schema_version\":{\"bsonType\":\"int\",\"minimum\":1}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "def main():\n",
    "    print(f\"[cfg] DB={DB_NAME} INCLUDE_DAILY_RHYTHM={INCLUDE_DAILY_RHYTHM}\")\n",
    "\n",
    "    # 1) CREATE collections first (no validators)\n",
    "    base = [\"users\", \"goals\", \"weekly_plans\", \"sessions\", \"reflections\", \"daily_targets\"]\n",
    "    for name in base:\n",
    "        create_if_missing(name)\n",
    "    if INCLUDE_DAILY_RHYTHM:\n",
    "        create_if_missing(\"daily_rhythm\")\n",
    "\n",
    "    # 2) APPLY validators via collMod\n",
    "    apply_validator(\"users\", users_validator)\n",
    "    apply_validator(\"goals\", goals_validator)\n",
    "    apply_validator(\"weekly_plans\", weekly_plans_validator)\n",
    "    apply_validator(\"sessions\", sessions_validator)\n",
    "    apply_validator(\"reflections\", reflections_validator)\n",
    "    apply_validator(\"daily_targets\", daily_targets_validator)\n",
    "    if INCLUDE_DAILY_RHYTHM:\n",
    "        apply_validator(\"daily_rhythm\", daily_rhythm_validator)\n",
    "\n",
    "    # 3) INDEXES\n",
    "    ensure_indexes()\n",
    "    print(\"[done] Collections created, validators applied, indexes ensured.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12a22afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBlockingIOError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/dns/query.py:651\u001b[0m, in \u001b[0;36m_udp_recv\u001b[0;34m(sock, max_size, expiration)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecvfrom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBlockingIOError\u001b[39;00m:\n",
      "\u001b[0;31mBlockingIOError\u001b[0m: [Errno 35] Resource temporarily unavailable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MONGO_URI \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DB_NAME:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set MONGO_URI and DB_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mMongoClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMONGO_URI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m db \u001b[38;5;241m=\u001b[39m client[DB_NAME]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# --------- helpers ---------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/pymongo/synchronous/mongo_client.py:784\u001b[0m, in \u001b[0;36mMongoClient.__init__\u001b[0;34m(self, host, port, document_class, tz_aware, connect, type_registry, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mvalidate_timeout_or_none_or_zero(\n\u001b[1;32m    782\u001b[0m         keyword_opts\u001b[38;5;241m.\u001b[39mcased_key(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnecttimeoutms\u001b[39m\u001b[38;5;124m\"\u001b[39m), timeout\n\u001b[1;32m    783\u001b[0m     )\n\u001b[0;32m--> 784\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43muri_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnect_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrv_service_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrv_service_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrv_max_hosts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrv_max_hosts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m seeds\u001b[38;5;241m.\u001b[39mupdate(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodelist\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    795\u001b[0m username \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m username\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/pymongo/uri_parser.py:558\u001b[0m, in \u001b[0;36mparse_uri\u001b[0;34m(uri, default_port, validate, warn, normalize, connect_timeout, srv_service_name, srv_max_hosts)\u001b[0m\n\u001b[1;32m    556\u001b[0m connect_timeout \u001b[38;5;241m=\u001b[39m connect_timeout \u001b[38;5;129;01mor\u001b[39;00m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnectTimeoutMS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    557\u001b[0m dns_resolver \u001b[38;5;241m=\u001b[39m _SrvResolver(fqdn, connect_timeout, srv_service_name, srv_max_hosts)\n\u001b[0;32m--> 558\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[43mdns_resolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hosts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m dns_options \u001b[38;5;241m=\u001b[39m dns_resolver\u001b[38;5;241m.\u001b[39mget_options()\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dns_options:\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/pymongo/srv_resolver.py:141\u001b[0m, in \u001b[0;36m_SrvResolver.get_hosts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_hosts\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m--> 141\u001b[0m     _, nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_srv_response_and_hosts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/pymongo/srv_resolver.py:120\u001b[0m, in \u001b[0;36m_SrvResolver._get_srv_response_and_hosts\u001b[0;34m(self, encapsulate_errors)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_srv_response_and_hosts\u001b[39m(\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m, encapsulate_errors: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[resolver\u001b[38;5;241m.\u001b[39mAnswer, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m--> 120\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencapsulate_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Construct address tuples\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    124\u001b[0m         (maybe_decode(res\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mto_text(omit_final_dot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)), res\u001b[38;5;241m.\u001b[39mport)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    126\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/pymongo/srv_resolver.py:106\u001b[0m, in \u001b[0;36m_SrvResolver._resolve_uri\u001b[0;34m(self, encapsulate_errors)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_resolve_uri\u001b[39m(\u001b[38;5;28mself\u001b[39m, encapsulate_errors: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m resolver\u001b[38;5;241m.\u001b[39mAnswer:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__srv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m._tcp.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__fqdn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSRV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlifetime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__connect_timeout\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encapsulate_errors:\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;66;03m# Raise the original error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/pymongo/srv_resolver.py:52\u001b[0m, in \u001b[0;36m_resolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resolver\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(resolver, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolve\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# dnspython >= 2\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# dnspython 1.X\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolver\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/dns/resolver.py:1564\u001b[0m, in \u001b[0;36mresolve\u001b[0;34m(qname, rdtype, rdclass, tcp, source, raise_on_no_answer, source_port, lifetime, search)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(\n\u001b[1;32m   1545\u001b[0m     qname: Union[dns\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mName, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   1546\u001b[0m     rdtype: Union[dns\u001b[38;5;241m.\u001b[39mrdatatype\u001b[38;5;241m.\u001b[39mRdataType, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m dns\u001b[38;5;241m.\u001b[39mrdatatype\u001b[38;5;241m.\u001b[39mA,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     search: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1554\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Answer:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Query nameservers to find the answer to the question.\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    This is a convenience function that uses the default resolver\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;124;03m    parameters.\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_default_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrdclass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtcp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_no_answer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlifetime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/dns/resolver.py:1322\u001b[0m, in \u001b[0;36mResolver.resolve\u001b[0;34m(self, qname, rdtype, rdclass, tcp, source, raise_on_no_answer, source_port, lifetime, search)\u001b[0m\n\u001b[1;32m   1320\u001b[0m timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_timeout(start, lifetime, resolution\u001b[38;5;241m.\u001b[39merrors)\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mnameserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtcp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   1330\u001b[0m     (_, done) \u001b[38;5;241m=\u001b[39m resolution\u001b[38;5;241m.\u001b[39mquery_result(\u001b[38;5;28;01mNone\u001b[39;00m, ex)\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/dns/nameserver.py:108\u001b[0m, in \u001b[0;36mDo53Nameserver.query\u001b[0;34m(self, request, timeout, source, source_port, max_size, one_rr_per_rrset, ignore_trailing)\u001b[0m\n\u001b[1;32m     97\u001b[0m     response \u001b[38;5;241m=\u001b[39m dns\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mtcp(\n\u001b[1;32m     98\u001b[0m         request,\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddress,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m         ignore_trailing\u001b[38;5;241m=\u001b[39mignore_trailing,\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mdns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mudp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_truncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mone_rr_per_rrset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_rr_per_rrset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_trailing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_trailing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_unexpected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/dns/query.py:873\u001b[0m, in \u001b[0;36mudp\u001b[0;34m(q, where, timeout, port, source, source_port, ignore_unexpected, one_rr_per_rrset, ignore_trailing, raise_on_truncation, sock, ignore_errors)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cm \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[1;32m    872\u001b[0m     send_udp(s, wire, destination, expiration)\n\u001b[0;32m--> 873\u001b[0m     (r, received_time) \u001b[38;5;241m=\u001b[39m \u001b[43mreceive_udp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpiration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_unexpected\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mone_rr_per_rrset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeyring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_trailing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_truncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m     r\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m=\u001b[39m received_time \u001b[38;5;241m-\u001b[39m begin_time\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;66;03m# We don't need to check q.is_response() if we are in ignore_errors mode\u001b[39;00m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# as receive_udp() will have checked it.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/dns/query.py:764\u001b[0m, in \u001b[0;36mreceive_udp\u001b[0;34m(sock, destination, expiration, ignore_unexpected, one_rr_per_rrset, keyring, request_mac, ignore_trailing, raise_on_truncation, ignore_errors, query)\u001b[0m\n\u001b[1;32m    762\u001b[0m wire \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m     (wire, from_address) \u001b[38;5;241m=\u001b[39m \u001b[43m_udp_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m65535\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpiration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _matches_destination(\n\u001b[1;32m    766\u001b[0m         sock\u001b[38;5;241m.\u001b[39mfamily, from_address, destination, ignore_unexpected\n\u001b[1;32m    767\u001b[0m     ):\n\u001b[1;32m    768\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/dns/query.py:653\u001b[0m, in \u001b[0;36m_udp_recv\u001b[0;34m(sock, max_size, expiration)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\u001b[38;5;241m.\u001b[39mrecvfrom(max_size)\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBlockingIOError\u001b[39;00m:\n\u001b[0;32m--> 653\u001b[0m     \u001b[43m_wait_for_readable\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpiration\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/dns/query.py:241\u001b[0m, in \u001b[0;36m_wait_for_readable\u001b[0;34m(s, expiration)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_for_readable\u001b[39m(s, expiration):\n\u001b[0;32m--> 241\u001b[0m     \u001b[43m_wait_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpiration\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/site-packages/dns/query.py:236\u001b[0m, in \u001b[0;36m_wait_for\u001b[0;34m(fd, readable, writable, _, expiration)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m dns\u001b[38;5;241m.\u001b[39mexception\u001b[38;5;241m.\u001b[39mTimeout\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43msel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m dns\u001b[38;5;241m.\u001b[39mexception\u001b[38;5;241m.\u001b[39mTimeout\n",
      "File \u001b[0;32m~/miniconda/miniconda3/envs/rachana/lib/python3.10/selectors.py:562\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Sanity Suite for Focus Timer DB (A‚ÄìF collections).\n",
    "\n",
    "Env:\n",
    "  MONGO_URI     (required)\n",
    "  DB_NAME       (required)\n",
    "  USER_FILTER   (optional; if set, checks only this user)\n",
    "  MAX_SCAN      (optional; default 200) -> sample size for key discovery\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "\n",
    "IST = pytz.timezone(\"Asia/Kolkata\")\n",
    "\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\", \"mongodb+srv://prashanth01071995:pradsml%402025@cluster0.fsbic.mongodb.net/\")\n",
    "DB_NAME = os.environ.get(\"DB_NAME\", \"Focus_DB\")\n",
    "USER_FILTER = os.environ.get(\"USER_FILTER\", \"\").strip() or None\n",
    "MAX_SCAN  = int(os.environ.get(\"MAX_SCAN\", \"200\"))\n",
    "\n",
    "if not MONGO_URI or not DB_NAME:\n",
    "    raise SystemExit(\"Please set MONGO_URI and DB_NAME\")\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "\n",
    "# --------- helpers ---------\n",
    "def iso_week_key_from_datestr(datestr: str) -> str:\n",
    "    # datestr: \"YYYY-MM-DD\"\n",
    "    y, m, d = map(int, datestr.split(\"-\"))\n",
    "    dt = datetime(y, m, d)\n",
    "    iso = dt.isocalendar()\n",
    "    return f\"{iso.year}-{iso.week:02d}\"\n",
    "\n",
    "def flatten(doc, prefix=\"\", paths=None):\n",
    "    if paths is None:\n",
    "        paths = []\n",
    "    if isinstance(doc, dict):\n",
    "        for k, v in doc.items():\n",
    "            flatten(v, f\"{prefix}{k}.\" if prefix else f\"{k}.\", paths)\n",
    "    elif isinstance(doc, list):\n",
    "        # treat list elements generically as []\n",
    "        paths.append((prefix.rstrip(\".\"), \"array\"))\n",
    "        for el in doc[:3]:  # inspect up to first 3 for nested keys\n",
    "            flatten(el, f\"{prefix}[]\".rstrip(\".\"), paths)\n",
    "    else:\n",
    "        # leaf\n",
    "        paths.append((prefix.rstrip(\".\"), type(doc).__name__))\n",
    "    return paths\n",
    "\n",
    "def type_label(v):\n",
    "    if v is None:\n",
    "        return \"null\"\n",
    "    t = type(v).__name__\n",
    "    return t\n",
    "\n",
    "def is_number(v):\n",
    "    return isinstance(v, (int, float)) and not isinstance(v, bool)\n",
    "\n",
    "def approx_equal(a, b, eps=0.75):\n",
    "    # generous window due to timer rounding and clock drift\n",
    "    try:\n",
    "        return abs(float(a) - float(b)) <= eps\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def pct(n, d):\n",
    "    return (n / d * 100.0) if d else 0.0\n",
    "\n",
    "def print_header(title):\n",
    "    bar = \"‚ïê\" * len(title)\n",
    "    print(f\"\\n{title}\\n{bar}\")\n",
    "\n",
    "def coll_exists(name): return name in db.list_collection_names()\n",
    "\n",
    "def users_in_scope():\n",
    "    if USER_FILTER:\n",
    "        return [USER_FILTER]\n",
    "    # else collect distinct users from any core collection\n",
    "    users = set()\n",
    "    for cname in [\"users\", \"goals\", \"weekly_plans\", \"sessions\", \"reflections\", \"daily_targets\"]:\n",
    "        if coll_exists(cname):\n",
    "            users.update(db.get_collection(cname).distinct(\"user\"))\n",
    "    return sorted(u for u in users if u)\n",
    "\n",
    "# --------- 0) Meta ---------\n",
    "print_header(f\"[cfg] DB={DB_NAME} USER_FILTER={USER_FILTER or '*'}\")\n",
    "print(\"Collections:\", \", \".join(sorted(db.list_collection_names())))\n",
    "\n",
    "# --------- 1) Print keys (schema discovery) ---------\n",
    "def schema_discovery(cname, sample=MAX_SCAN):\n",
    "    if not coll_exists(cname):\n",
    "        print(f\"[warn] collection missing: {cname}\")\n",
    "        return\n",
    "    print_header(f\"[keys] {cname}\")\n",
    "    cur = db.get_collection(cname).find({}, limit=sample)\n",
    "    paths = defaultdict(lambda: {\"count\": 0, \"types\": Counter()})\n",
    "    total = 0\n",
    "    for doc in cur:\n",
    "        total += 1\n",
    "        seen = set()\n",
    "        for path, t in flatten(doc):\n",
    "            if path not in seen:\n",
    "                paths[path][\"count\"] += 1\n",
    "                seen.add(path)\n",
    "            paths[path][\"types\"][t] += 1\n",
    "    print(f\"sampled docs: {total}\")\n",
    "    for p, info in sorted(paths.items()):\n",
    "        types = \", \".join(f\"{t}:{c}\" for t, c in info[\"types\"].most_common())\n",
    "        print(f\"  {p}: {info['count']}/{total}  [{types}]\")\n",
    "\n",
    "for cname in [\"users\", \"goals\", \"weekly_plans\", \"sessions\", \"reflections\", \"daily_targets\"]:\n",
    "    schema_discovery(cname)\n",
    "\n",
    "# --------- 2) Required-field sanity / regex ---------\n",
    "def regex_checks():\n",
    "    print_header(\"[regex] key formats\")\n",
    "    # weekly_plans\n",
    "    if coll_exists(\"weekly_plans\"):\n",
    "        bad_week_keys = list(db.weekly_plans.find(\n",
    "            {\"week_key\": {\"$not\": re.compile(r\"^\\d{4}-\\d{2}$\")}},\n",
    "            {\"_id\": 1, \"week_key\": 1}\n",
    "        ))\n",
    "        print(\"weekly_plans.week_key bad:\", len(bad_week_keys))\n",
    "        # dates\n",
    "        bad_dates = list(db.weekly_plans.find(\n",
    "            {\"$or\": [\n",
    "                {\"week_start\": {\"$not\": re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")}},\n",
    "                {\"week_end\": {\"$not\": re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")}},\n",
    "            ]},\n",
    "            {\"_id\": 1, \"week_start\": 1, \"week_end\": 1}\n",
    "        ))\n",
    "        print(\"weekly_plans.week_start/week_end bad:\", len(bad_dates))\n",
    "    # sessions / reflections / daily_targets\n",
    "    for cname in [\"sessions\", \"reflections\", \"daily_targets\"]:\n",
    "        if not coll_exists(cname): continue\n",
    "        bad_date = list(db.get_collection(cname).find(\n",
    "            {\"date_ist\": {\"$not\": re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")}},\n",
    "            {\"_id\": 1, \"date_ist\": 1}\n",
    "        ))\n",
    "        print(f\"{cname}.date_ist bad:\", len(bad_date))\n",
    "regex_checks()\n",
    "\n",
    "# --------- 3) Referential integrity ---------\n",
    "def referential_checks():\n",
    "    print_header(\"[refs] goals referenced by plans/sessions exist\")\n",
    "    goal_ids = set()\n",
    "    if coll_exists(\"goals\"):\n",
    "        goal_ids = set(db.goals.distinct(\"_id\"))\n",
    "    unknown_plan = []\n",
    "    unknown_sess = []\n",
    "    if coll_exists(\"weekly_plans\"):\n",
    "        for row in db.weekly_plans.find({}, {\"_id\":1, \"items.goal_id\":1}):\n",
    "            for it in row.get(\"items\", []):\n",
    "                gid = it.get(\"goal_id\")\n",
    "                if gid and gid not in goal_ids:\n",
    "                    unknown_plan.append((row[\"_id\"], gid))\n",
    "    if coll_exists(\"sessions\"):\n",
    "        for s in db.sessions.find({\"goal_id\": {\"$ne\": None}}, {\"_id\":1, \"goal_id\":1}):\n",
    "            if s.get(\"goal_id\") not in goal_ids:\n",
    "                unknown_sess.append((s[\"_id\"], s.get(\"goal_id\")))\n",
    "    print(\"unknown goal_ids in weekly_plans:\", len(unknown_plan))\n",
    "    print(\"unknown goal_ids in sessions:\", len(unknown_sess))\n",
    "\n",
    "    # duplicate goal rows inside a single weekly plan\n",
    "    dup_rows = []\n",
    "    if coll_exists(\"weekly_plans\"):\n",
    "        for row in db.weekly_plans.find({}, {\"_id\":1, \"items.goal_id\":1}):\n",
    "            gids = [it.get(\"goal_id\") for it in row.get(\"items\", []) if it.get(\"goal_id\")]\n",
    "            if len(gids) != len(set(gids)):\n",
    "                dup_rows.append(row[\"_id\"])\n",
    "    print(\"duplicate goal rows in a plan:\", len(dup_rows))\n",
    "referential_checks()\n",
    "\n",
    "# --------- 4) Allocation math invariants ---------\n",
    "def allocation_math_checks():\n",
    "    print_header(\"[math] allocation invariants\")\n",
    "    if not coll_exists(\"weekly_plans\"):\n",
    "        print(\"no weekly_plans\")\n",
    "        return\n",
    "    bad_sum = []\n",
    "    bad_total = []\n",
    "    for row in db.weekly_plans.find({}, {\"_id\":1, \"capacity\":1, \"items\":1}):\n",
    "        cap = (row.get(\"capacity\") or {}).get(\"total\", 0)\n",
    "        planned_sum = sum(int(it.get(\"planned_current\", 0)) for it in row.get(\"items\", []))\n",
    "        if planned_sum != cap:\n",
    "            bad_sum.append((row[\"_id\"], planned_sum, cap))\n",
    "        for it in row.get(\"items\", []):\n",
    "            pc = int(it.get(\"planned_current\", 0))\n",
    "            bi = int(it.get(\"backlog_in\", 0))\n",
    "            tt = int(it.get(\"total_target\", 0))\n",
    "            if pc + bi != tt:\n",
    "                bad_total.append((row[\"_id\"], it.get(\"goal_id\"), pc, bi, tt))\n",
    "    print(\"Sum(planned_current) == capacity.total violations:\", len(bad_sum))\n",
    "    print(\"total_target == planned_current + backlog_in violations:\", len(bad_total))\n",
    "allocation_math_checks()\n",
    "\n",
    "# --------- 5) Alloc-bucket enforcement ---------\n",
    "def alloc_bucket_checks():\n",
    "    print_header(\"[bucket] current-before-backlog enforcement\")\n",
    "    if not (coll_exists(\"sessions\") and coll_exists(\"weekly_plans\")):\n",
    "        print(\"need sessions + weekly_plans\")\n",
    "        return\n",
    "\n",
    "    violations = []\n",
    "    # Group by user, week, goal\n",
    "    users = users_in_scope()\n",
    "    for U in users:\n",
    "        weeks = db.sessions.distinct(\"week_key\", {\"user\": U, \"t\": \"W\"})\n",
    "        for W in weeks:\n",
    "            # load plan items for this week\n",
    "            plan = db.weekly_plans.find_one({\"user\": U, \"week_key\": W}, {\"items\":1})\n",
    "            planned_by_goal = {}\n",
    "            if plan:\n",
    "                for it in plan.get(\"items\", []):\n",
    "                    planned_by_goal[it.get(\"goal_id\")] = int(it.get(\"planned_current\", 0))\n",
    "            # group sessions\n",
    "            pipeline = [\n",
    "                {\"$match\": {\"user\": U, \"week_key\": W, \"t\": \"W\"}},\n",
    "                {\"$group\": {\"_id\": {\"goal_id\": \"$goal_id\", \"bucket\": \"$alloc_bucket\"},\n",
    "                            \"count\": {\"$sum\": 1}}}\n",
    "            ]\n",
    "            bucket_map = defaultdict(lambda: {\"current\": 0, \"backlog\": 0})\n",
    "            for r in db.sessions.aggregate(pipeline):\n",
    "                gid = r[\"_id\"][\"goal_id\"]\n",
    "                b   = r[\"_id\"][\"bucket\"] or \"current\"\n",
    "                bucket_map[gid][b] += r[\"count\"]\n",
    "            # checks\n",
    "            for gid, cts in bucket_map.items():\n",
    "                pc = planned_by_goal.get(gid, 0)\n",
    "                cur = cts.get(\"current\", 0)\n",
    "                back = cts.get(\"backlog\", 0)\n",
    "                if cur > pc:\n",
    "                    violations.append((U, W, gid, f\"done_current={cur} > planned_current={pc}\"))\n",
    "                if cur < pc and back > 0:\n",
    "                    violations.append((U, W, gid, f\"backlog used ({back}) before clearing current ({cur}/{pc})\"))\n",
    "    print(\"alloc-bucket rule violations:\", len(violations))\n",
    "    for v in violations[:20]:\n",
    "        print(\"  \", v)\n",
    "alloc_bucket_checks()\n",
    "\n",
    "# --------- 6) Carry-forward consistency ---------\n",
    "def carry_forward_checks():\n",
    "    print_header(\"[rollover] carry-forward prev‚Üíthis week\")\n",
    "    if not (coll_exists(\"sessions\") and coll_exists(\"weekly_plans\")):\n",
    "        print(\"need sessions + weekly_plans\")\n",
    "        return\n",
    "\n",
    "    warn, err = [], []\n",
    "    users = users_in_scope()\n",
    "    for U in users:\n",
    "        weeks = sorted(db.weekly_plans.distinct(\"week_key\", {\"user\": U}))\n",
    "        for i in range(1, len(weeks)):\n",
    "            prev, curr = weeks[i-1], weeks[i]\n",
    "            prev_plan = db.weekly_plans.find_one({\"user\": U, \"week_key\": prev}, {\"items\":1})\n",
    "            curr_plan = db.weekly_plans.find_one({\"user\": U, \"week_key\": curr}, {\"items\":1})\n",
    "            if not prev_plan or not curr_plan: continue\n",
    "\n",
    "            # actual totals by goal in prev\n",
    "            actual_prev = defaultdict(int)\n",
    "            for r in db.sessions.aggregate([\n",
    "                {\"$match\": {\"user\": U, \"week_key\": prev, \"t\": \"W\"}},\n",
    "                {\"$group\": {\"_id\": \"$goal_id\", \"cnt\": {\"$sum\": 1}}}\n",
    "            ]):\n",
    "                actual_prev[r[\"_id\"]] = r[\"cnt\"]\n",
    "\n",
    "            back_in_curr = {it.get(\"goal_id\"): int(it.get(\"backlog_in\", 0)) for it in curr_plan.get(\"items\", [])}\n",
    "            items_prev = prev_plan.get(\"items\", [])\n",
    "\n",
    "            for it in items_prev:\n",
    "                gid = it.get(\"goal_id\")\n",
    "                total_target = int(it.get(\"total_target\", 0))\n",
    "                actual = actual_prev.get(gid, 0)\n",
    "                close_action = it.get(\"close_action\")\n",
    "                carryover_out = max(total_target - actual, 0)\n",
    "                if close_action == \"Completed\":\n",
    "                    expected = 0\n",
    "                    # if present in curr and backlog_in>0, error\n",
    "                    if back_in_curr.get(gid, 0) != expected and gid in back_in_curr:\n",
    "                        err.append((U, prev, curr, gid, f\"Completed last week, expected backlog_in=0, got {back_in_curr.get(gid)}\"))\n",
    "                elif close_action == \"Continue\" or close_action is None:\n",
    "                    expected = carryover_out\n",
    "                    # if goal present in curr, compare; if absent but expected>0, warn\n",
    "                    if gid in back_in_curr:\n",
    "                        if back_in_curr[gid] != expected:\n",
    "                            err.append((U, prev, curr, gid, f\"expected backlog_in={expected}, got {back_in_curr[gid]}\"))\n",
    "                    else:\n",
    "                        if expected > 0:\n",
    "                            warn.append((U, prev, curr, gid, f\"carryover_out={expected} but goal not in next-week plan\"))\n",
    "                elif close_action == \"On Hold\":\n",
    "                    # policy: parking is allowed; warn if backlog carried silently\n",
    "                    if gid in back_in_curr and back_in_curr[gid] > 0:\n",
    "                        warn.append((U, prev, curr, gid, f\"On Hold but backlog_in carried ({back_in_curr[gid]})\"))\n",
    "                else:\n",
    "                    # unknown close_action -> warn\n",
    "                    warn.append((U, prev, curr, gid, f\"unknown close_action={close_action}\"))\n",
    "    print(\"carry-forward ERR:\", len(err))\n",
    "    for e in err[:20]:\n",
    "        print(\"  \", e)\n",
    "    print(\"carry-forward WARN:\", len(warn))\n",
    "    for w in warn[:20]:\n",
    "        print(\"  \", w)\n",
    "carry_forward_checks()\n",
    "\n",
    "# --------- 7) Session integrity ---------\n",
    "def session_integrity_checks():\n",
    "    print_header(\"[sessions] integrity\")\n",
    "    if not coll_exists(\"sessions\"):\n",
    "        print(\"no sessions\")\n",
    "        return\n",
    "    # Timing & duration\n",
    "    bad_time = 0\n",
    "    bad_dur = 0\n",
    "    bad_equiv = 0\n",
    "    bad_kind = 0\n",
    "    cursor = db.sessions.find({}, {\"_id\":1,\"t\":1,\"kind\":1,\"activity_type\":1,\"started_at_ist\":1,\"ended_at_ist\":1,\"dur_min\":1,\"pom_equiv\":1})\n",
    "    for s in cursor:\n",
    "        st, en = s.get(\"started_at_ist\"), s.get(\"ended_at_ist\")\n",
    "        dur = s.get(\"dur_min\")\n",
    "        if isinstance(st, datetime) and isinstance(en, datetime):\n",
    "            if en <= st: bad_time += 1\n",
    "            mins = (en - st).total_seconds() / 60.0\n",
    "            if not approx_equal(mins, dur, eps=2.0):  # allow a 2-min window\n",
    "                bad_dur += 1\n",
    "        # pom_equiv\n",
    "        pe = s.get(\"pom_equiv\")\n",
    "        if pe is not None and dur is not None:\n",
    "            if not approx_equal(pe, dur / 25.0, eps=0.2):\n",
    "                bad_equiv += 1\n",
    "        # kind/activity_type coherence\n",
    "        kind = s.get(\"kind\")\n",
    "        atype = s.get(\"activity_type\")\n",
    "        if kind == \"activity\" and (atype not in [ \"exercise\", \"meditation\", \"breathing\", \"other\" ]):\n",
    "            bad_kind += 1\n",
    "    print(\"ended_at > started_at violations:\", bad_time)\n",
    "    print(\"dur_min mismatch to timestamps:\", bad_dur)\n",
    "    print(\"pom_equiv != dur_min/25 mismatches:\", bad_equiv)\n",
    "    print(\"activity sessions missing/invalid activity_type:\", bad_kind)\n",
    "session_integrity_checks()\n",
    "\n",
    "# --------- 8) Date/IST alignment for week_key ---------\n",
    "def week_key_alignment():\n",
    "    print_header(\"[week_key] aligns with date_ist\")\n",
    "    if not coll_exists(\"sessions\"):\n",
    "        print(\"no sessions\")\n",
    "        return\n",
    "    mismatches = []\n",
    "    for s in db.sessions.find({}, {\"_id\":1,\"date_ist\":1,\"week_key\":1}).limit(5000):\n",
    "        ds = s.get(\"date_ist\")\n",
    "        wk = s.get(\"week_key\")\n",
    "        try:\n",
    "            calc = iso_week_key_from_datestr(ds)\n",
    "            if wk != calc:\n",
    "                mismatches.append((s[\"_id\"], ds, wk, calc))\n",
    "        except Exception:\n",
    "            mismatches.append((s[\"_id\"], ds, wk, \"bad-date\"))\n",
    "    print(\"week_key!=iso(date_ist) mismatches:\", len(mismatches))\n",
    "    for m in mismatches[:20]:\n",
    "        print(\"  \", m)\n",
    "week_key_alignment()\n",
    "\n",
    "# --------- 9) Analytics readiness (weekly KPIs) ---------\n",
    "def weekly_kpis():\n",
    "    print_header(\"[kpi] weekly metrics (pom_equiv & focus-only deep-work)\")\n",
    "    if not (coll_exists(\"sessions\") and coll_exists(\"weekly_plans\")):\n",
    "        print(\"need sessions + weekly_plans\")\n",
    "        return\n",
    "    for U in users_in_scope():\n",
    "        weeks = sorted(db.sessions.distinct(\"week_key\", {\"user\": U, \"t\": \"W\"}))\n",
    "        if not weeks:\n",
    "            continue\n",
    "        print(f\"User: {U}\")\n",
    "        print(f\"{'Week':<8} {'Planned':>7} {'ActualPE':>9} {'Adh%':>6} | {'Deep%':>6} {'Break%':>7} {'Unplan%':>8}\")\n",
    "        for W in weeks:\n",
    "            plan = db.weekly_plans.find_one({\"user\": U, \"week_key\": W}, {\"items\":1})\n",
    "            planned = sum(int(it.get(\"planned_current\", 0)) for it in (plan.get(\"items\", []) if plan else []))\n",
    "\n",
    "            # actual pom-equivalents\n",
    "            pe_doc = next(iter(db.sessions.aggregate([\n",
    "                {\"$match\": {\"user\": U, \"week_key\": W, \"t\": \"W\"}},\n",
    "                {\"$group\": {\"_id\": None, \"pe\": {\"$sum\": {\"$ifNull\": [\"$pom_equiv\", {\"$divide\": [\"$dur_min\", 25.0]}]}}}}\n",
    "            ])), None)\n",
    "            actual_pe = float(pe_doc[\"pe\"]) if pe_doc else 0.0\n",
    "\n",
    "            # deep-work %: focus-only\n",
    "            focus_total = db.sessions.count_documents({\"user\": U, \"week_key\": W, \"t\": \"W\", \"kind\": {\"$ne\": \"activity\"}})\n",
    "            deep = db.sessions.count_documents({\"user\": U, \"week_key\": W, \"t\": \"W\", \"kind\": {\"$ne\": \"activity\"}, \"deep_work\": True})\n",
    "            deep_pct = pct(deep, focus_total)\n",
    "\n",
    "            # break compliance: valid breaks vs focus sessions\n",
    "            valid_breaks = db.sessions.count_documents({\n",
    "                \"user\": U, \"week_key\": W, \"t\": \"B\",\n",
    "                \"dur_min\": {\"$gte\": 4},\n",
    "                \"$or\": [{\"skipped\": {\"$exists\": False}}, {\"skipped\": {\"$ne\": True}}]\n",
    "            })\n",
    "            break_pct = pct(min(valid_breaks, focus_total), focus_total)\n",
    "\n",
    "            # unplanned % by pom-equivalents\n",
    "            pe_by_mode = {row[\"_id\"]: row[\"pe\"] for row in db.sessions.aggregate([\n",
    "                {\"$match\": {\"user\": U, \"week_key\": W, \"t\": \"W\"}},\n",
    "                {\"$group\": {\"_id\": \"$goal_mode\", \"pe\": {\"$sum\": {\"$ifNull\": [\"$pom_equiv\", {\"$divide\": [\"$dur_min\", 25.0]}]}}}}\n",
    "            ])}\n",
    "            unplanned_pe = float(pe_by_mode.get(\"custom\", 0.0))\n",
    "            unplanned_pct = pct(unplanned_pe, actual_pe)\n",
    "\n",
    "            adh = pct(min(actual_pe, planned), planned) if planned else 0.0\n",
    "            print(f\"{W:<8} {planned:7d} {actual_pe:9.1f} {adh:6.1f} | {deep_pct:6.1f} {break_pct:7.1f} {unplanned_pct:8.1f}\")\n",
    "\n",
    "weekly_kpis()\n",
    "\n",
    "# --------- 10) Wellbeing balance (career vs wellbeing) ---------\n",
    "def balance_split():\n",
    "    print_header(\"[balance] Career vs Wellbeing split (pom_equiv)\")\n",
    "    if not coll_exists(\"sessions\"):\n",
    "        print(\"no sessions\")\n",
    "        return\n",
    "    # build goal->domain map\n",
    "    goal_domain = {}\n",
    "    if coll_exists(\"goals\"):\n",
    "        for g in db.goals.find({}, {\"_id\":1, \"category\":1}):\n",
    "            cat = (g.get(\"category\") or \"\").lower()\n",
    "            if cat in [\"health\", \"wellbeing\"]:\n",
    "                goal_domain[g[\"_id\"]] = \"Wellbeing\"\n",
    "            else:\n",
    "                goal_domain[g[\"_id\"]] = \"Career\"\n",
    "    for U in users_in_scope():\n",
    "        weeks = sorted(db.sessions.distinct(\"week_key\", {\"user\": U, \"t\": \"W\"}))\n",
    "        if not weeks:\n",
    "            continue\n",
    "        print(f\"User: {U}\")\n",
    "        for W in weeks:\n",
    "            career = 0.0\n",
    "            well   = 0.0\n",
    "            for s in db.sessions.find({\"user\": U, \"week_key\": W, \"t\": \"W\"}, {\"goal_id\":1,\"kind\":1,\"activity_type\":1,\"pom_equiv\":1}):\n",
    "                pe = s.get(\"pom_equiv\", 0.0) or 0.0\n",
    "                gid = s.get(\"goal_id\")\n",
    "                kind = s.get(\"kind\")\n",
    "                if kind == \"activity\":\n",
    "                    well += pe\n",
    "                else:\n",
    "                    dom = goal_domain.get(gid, \"Career\")\n",
    "                    if dom == \"Wellbeing\": well += pe\n",
    "                    else: career += pe\n",
    "            total = career + well\n",
    "            cp = pct(career, total)\n",
    "            wp = pct(well, total)\n",
    "            print(f\"  {W}: Career {cp:5.1f}% | Wellbeing {wp:5.1f}%  (total={total:.1f} pe)\")\n",
    "balance_split()\n",
    "\n",
    "# --------- 11) Index coverage ---------\n",
    "def list_indexes():\n",
    "    print_header(\"[indexes] per collection\")\n",
    "    for cname in [\"users\",\"goals\",\"weekly_plans\",\"sessions\",\"reflections\",\"daily_targets\",\"daily_rhythm\"]:\n",
    "        if not coll_exists(cname):\n",
    "            continue\n",
    "        print(f\"{cname}:\")\n",
    "        try:\n",
    "            for ix in db.get_collection(cname).list_indexes():\n",
    "                # ix is a SON/dict\n",
    "                name = ix.get(\"name\")\n",
    "                key  = dict(ix.get(\"key\", {}))\n",
    "                unique = ix.get(\"unique\", False)\n",
    "                print(f\"  - {name} : {key} {'(unique)' if unique else ''}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [warn] couldn't list indexes: {e}\")\n",
    "\n",
    "\n",
    "# --------- 12) Orphan weeks ---------\n",
    "def orphans():\n",
    "    print_header(\"[orphans] sessions w/o plan; plans w/o sessions\")\n",
    "    users = users_in_scope()\n",
    "    for U in users:\n",
    "        ses_weeks = set(db.sessions.distinct(\"week_key\", {\"user\": U})) if coll_exists(\"sessions\") else set()\n",
    "        plan_weeks = set(db.weekly_plans.distinct(\"week_key\", {\"user\": U})) if coll_exists(\"weekly_plans\") else set()\n",
    "        only_ses = sorted(ses_weeks - plan_weeks)\n",
    "        only_plan = sorted(plan_weeks - ses_weeks)\n",
    "        print(f\"User: {U} | weeks in sessions: {len(ses_weeks)} | weeks in plans: {len(plan_weeks)}\")\n",
    "        if only_ses:\n",
    "            print(\"  sessions-only weeks:\", \", \".join(only_ses[:20]))\n",
    "        if only_plan:\n",
    "            print(\"  plans-only weeks   :\", \", \".join(only_plan[:20]))\n",
    "orphans()\n",
    "\n",
    "print_header(\"[done] sanity suite finished\")\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def plan_vs_progress(user, week):\n",
    "    print_header(f\"[week detail] {user} {week} ‚Äî plan vs progress\")\n",
    "    plan = db.weekly_plans.find_one({\"user\": user, \"week_key\": week})\n",
    "    if not plan:\n",
    "        print(\"no plan\")\n",
    "        return\n",
    "    items = {it[\"goal_id\"]: it for it in plan.get(\"items\", [])}\n",
    "\n",
    "    agg = list(db.sessions.aggregate([\n",
    "        {\"$match\": {\"user\": user, \"week_key\": week, \"t\":\"W\"}},\n",
    "        {\"$group\": {\"_id\": {\"goal_id\":\"$goal_id\",\"bucket\":\"$alloc_bucket\"},\n",
    "                    \"cnt\": {\"$sum\": 1},\n",
    "                    \"pe\": {\"$sum\": {\"$ifNull\": [\"$pom_equiv\", {\"$divide\": [\"$dur_min\", 25.0]}]}}}}\n",
    "    ]))\n",
    "    done = defaultdict(lambda: {\"current_cnt\":0,\"backlog_cnt\":0,\"current_pe\":0.0,\"backlog_pe\":0.0})\n",
    "    for r in agg:\n",
    "        gid = r[\"_id\"][\"goal_id\"]\n",
    "        b = (r[\"_id\"][\"bucket\"] or \"current\")\n",
    "        if b == \"current\":\n",
    "            done[gid][\"current_cnt\"] += r[\"cnt\"]; done[gid][\"current_pe\"] += r[\"pe\"]\n",
    "        else:\n",
    "            done[gid][\"backlog_cnt\"] += r[\"cnt\"]; done[gid][\"backlog_pe\"] += r[\"pe\"]\n",
    "\n",
    "    print(f\"{'Goal':<16} {'CurCnt':>6} {'BackCnt':>7} | {'Planned':>7} {'BackIn':>6} {'Total':>6} || {'CurPE':>6} {'BackPE':>6}\")\n",
    "    for gid, it in items.items():\n",
    "        d = done.get(gid, {})\n",
    "        print(f\"{gid:<16} {d.get('current_cnt',0):6d} {d.get('backlog_cnt',0):7d} | \"\n",
    "              f\"{it['planned_current']:7d} {it['backlog_in']:6d} {it['total_target']:6d} || \"\n",
    "              f\"{d.get('current_pe',0.0):6.1f} {d.get('backlog_pe',0.0):6.1f}\")\n",
    "\n",
    "for U in users_in_scope():\n",
    "    weeks = sorted(db.weekly_plans.distinct(\"week_key\", {\"user\": U}))\n",
    "    if weeks:\n",
    "        plan_vs_progress(U, weeks[-1])\n",
    "\n",
    "def daily_snapshot(user, datestr):\n",
    "    print_header(f\"[daily] {user} {datestr}\")\n",
    "    # target\n",
    "    tgt = db.daily_targets.find_one({\"user\": user, \"date_ist\": datestr})\n",
    "    target = tgt.get(\"target_pomos\") if tgt else None\n",
    "    # sessions\n",
    "    sess = list(db.sessions.find({\"user\": user, \"date_ist\": datestr}).sort(\"started_at_ist\", 1))\n",
    "    first = sess[0][\"started_at_ist\"].astimezone(IST).strftime(\"%H:%M\") if sess else None\n",
    "    pe = sum((s.get(\"pom_equiv\") or (s.get(\"dur_min\",0)/25.0)) for s in sess if s.get(\"t\")==\"W\")\n",
    "    focus_total = sum(1 for s in sess if s.get(\"t\")==\"W\" and s.get(\"kind\")!=\"activity\")\n",
    "    valid_breaks = sum(1 for s in sess if s.get(\"t\")==\"B\" and (s.get(\"dur_min\",0)>=4) and not s.get(\"skipped\", False))\n",
    "    break_pct = pct(min(valid_breaks, focus_total), focus_total)\n",
    "    # reflection\n",
    "    refl = db.reflections.find_one({\"user\": user, \"date_ist\": datestr})\n",
    "    refl_ok = bool(refl and refl.get(\"reflection_submitted\"))\n",
    "    print(f\"Target: {target} poms | ActualPE: {pe:.1f} | First session: {first} | Break%: {break_pct:.1f} | Reflection: {'‚úî' if refl_ok else '‚Äî'}\")\n",
    "\n",
    "for U in users_in_scope():\n",
    "    for D in sorted(db.sessions.distinct(\"date_ist\", {\"user\": U}))[-3:]:\n",
    "        daily_snapshot(U, D)\n",
    "\n",
    "def balance_band_check(user, week):\n",
    "    print_header(f\"[balance band] {user} {week}\")\n",
    "    u = db.users.find_one({\"_id\": user}) or {}\n",
    "    band = ((u.get(\"prefs\") or {}).get(\"balance_band\") or {})\n",
    "    target = float(band.get(\"career_target_pct\", 70))\n",
    "    tol = float(band.get(\"tolerance_pct\", 10))\n",
    "\n",
    "    # goal -> domain\n",
    "    goal_domain = {}\n",
    "    for g in db.goals.find({\"user\": user}, {\"_id\":1, \"category\":1}):\n",
    "        goal_domain[g[\"_id\"]] = \"Wellbeing\" if (g.get(\"category\",\"\").lower() in [\"health\",\"wellbeing\"]) else \"Career\"\n",
    "\n",
    "    career = 0.0\n",
    "    wellbeing = 0.0\n",
    "    for s in db.sessions.find({\"user\": user, \"week_key\": week, \"t\":\"W\"},\n",
    "                              {\"goal_id\":1,\"kind\":1,\"pom_equiv\":1,\"dur_min\":1}):\n",
    "        pe = s.get(\"pom_equiv\")\n",
    "        if pe is None:\n",
    "            pe = (s.get(\"dur_min\", 0) / 25.0)\n",
    "\n",
    "        if s.get(\"kind\") == \"activity\":\n",
    "            wellbeing += pe\n",
    "        else:\n",
    "            dom = goal_domain.get(s.get(\"goal_id\"), \"Career\")\n",
    "            if dom == \"Wellbeing\":\n",
    "                wellbeing += pe\n",
    "            else:\n",
    "                career += pe\n",
    "\n",
    "    total = career + wellbeing\n",
    "    if total == 0:\n",
    "        print(\"No work/activity to evaluate this week.\")\n",
    "        return\n",
    "\n",
    "    career_pct = pct(career, total)\n",
    "    lo, hi = target - tol, target + tol\n",
    "    in_band = (lo <= career_pct <= hi)\n",
    "    print(f\"Career {career_pct:.1f}% vs target {target:.0f}%¬±{tol:.0f}% ‚Üí {'OK' if in_band else 'OUT OF BAND'}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6777a8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[seed] done: {'user': 'prashanth', 'date_ist': '2025-08-28', 'week_key': '2025-35', 'plan_id': 'prashanth|2025-35'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, math\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\", \"mongodb+srv://prashanth01071995:pradsml%402025@cluster0.fsbic.mongodb.net/\")\n",
    "DB_NAME = os.environ.get(\"DB_NAME\", \"Focus_DB\")\n",
    "USER      = os.environ.get(\"USER_ID\", \"prashanth\")\n",
    "if not MONGO_URI: raise SystemExit(\"Set MONGO_URI\")\n",
    "\n",
    "IST = pytz.timezone(\"Asia/Kolkata\")\n",
    "now_ist = datetime.now(IST)\n",
    "date_ist = now_ist.date().isoformat()\n",
    "\n",
    "# ISO week (IST)\n",
    "iso = now_ist.isocalendar()  # (year, week, weekday)\n",
    "week_key = f\"{iso.year}-{iso.week:02d}\"\n",
    "monday_ist = (now_ist - timedelta(days=iso.weekday-1)).date() if hasattr(iso,'weekday') else (now_ist - timedelta(days=now_ist.isoweekday()-1)).date()\n",
    "# Py>=3.11 has isocalendar().weekday; fallback above\n",
    "monday_ist = (now_ist - timedelta(days=now_ist.isoweekday()-1)).date()\n",
    "sunday_ist = monday_ist + timedelta(days=6)\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "\n",
    "def upsert(coll, _id, doc):\n",
    "    db[coll].update_one({\"_id\": _id}, {\"$setOnInsert\": doc}, upsert=True)\n",
    "\n",
    "UTCNOW = datetime.now(timezone.utc)\n",
    "\n",
    "# 1) user\n",
    "upsert(\"users\", USER, {\n",
    "  \"_id\": USER,\n",
    "  \"email\": f\"{USER}@example.com\",\n",
    "  \"tz\": \"Asia/Kolkata\",\n",
    "  \"prefs\": {\n",
    "    \"sound\": True,\n",
    "    \"auto_break\": True,\n",
    "    \"preferred_start_window\": {\"start\": \"06:00\", \"end\": \"08:00\"},\n",
    "    \"rank_weight_map\": {\"1\":5,\"2\":3,\"3\":2,\"4\":1,\"5\":1},\n",
    "    \"balance_band\": {\"career_target_pct\": 70, \"tolerance_pct\": 10},\n",
    "    \"pom_default\": {\"work_min\": 25, \"break_min\": 5}\n",
    "  },\n",
    "  \"created_at\": UTCNOW, \"updated_at\": UTCNOW,\n",
    "  \"schema_version\": 1\n",
    "})\n",
    "\n",
    "# 2) goals (3 examples)\n",
    "goals = [\n",
    "  (\"g_learn\", {\"title\":\"MongoDB Developer skills\",\"category\":\"Learning\",\"status\":\"In Progress\",\"is_primary\":True}),\n",
    "  (\"g_proj\" , {\"title\":\"Portfolio site & Blogs\",\"category\":\"Projects\",\"status\":\"In Progress\",\"is_primary\":True}),\n",
    "  (\"g_well\" , {\"title\":\"Morning Exercise\",\"category\":\"Health\",\"status\":\"In Progress\",\"is_primary\":False}),\n",
    "]\n",
    "for gid, g in goals:\n",
    "    upsert(\"goals\", gid, {\n",
    "      \"_id\": gid, \"user\": USER, **g,\n",
    "      \"tags\": [], \"target_poms\": None,\n",
    "      \"created_at\": UTCNOW, \"updated_at\": UTCNOW, \"schema_version\":1\n",
    "    })\n",
    "\n",
    "# 3) weekly plan (Current/Backlog/Total)\n",
    "capacity = {\"weekday\":3,\"weekend\":6,\"total\":27}\n",
    "weights = [5,3,2]  # rank1, rank2, rank3\n",
    "gids = [\"g_learn\",\"g_proj\",\"g_well\"]\n",
    "shares = [capacity[\"total\"]*w/sum(weights) for w in weights]\n",
    "base = [int(x) for x in shares]\n",
    "left = capacity[\"total\"] - sum(base)\n",
    "fract = sorted(list(enumerate([s - int(s) for s in shares])), key=lambda p: p[1], reverse=True)\n",
    "for i in range(left): base[fract[i][0]] += 1\n",
    "planned_current = dict(zip(gids, base))  # e.g., 14/8/5\n",
    "\n",
    "backlog_in = {\"g_learn\": 2, \"g_proj\": 0, \"g_well\": 0}\n",
    "\n",
    "items = []\n",
    "for i, gid in enumerate(gids):\n",
    "    pc = int(planned_current[gid])\n",
    "    bi = int(backlog_in[gid])\n",
    "    items.append({\n",
    "      \"goal_id\": gid,\n",
    "      \"priority_rank\": i+1,\n",
    "      \"weight\": weights[i],\n",
    "      \"planned_current\": pc,\n",
    "      \"backlog_in\": bi,\n",
    "      \"total_target\": pc + bi,\n",
    "      \"status_at_plan\": \"In Progress\",\n",
    "      \"close_action\": None,\n",
    "      \"notes\": None\n",
    "    })\n",
    "\n",
    "plan_id = f\"{USER}|{week_key}\"\n",
    "upsert(\"weekly_plans\", plan_id, {\n",
    "  \"_id\": plan_id,\n",
    "  \"user\": USER,\n",
    "  \"week_key\": week_key,\n",
    "  \"week_start\": monday_ist.isoformat(),\n",
    "  \"week_end\": sunday_ist.isoformat(),\n",
    "  \"capacity\": capacity,\n",
    "  \"items\": items,\n",
    "  \"created_at\": UTCNOW, \"updated_at\": UTCNOW,\n",
    "  \"schema_version\": 1\n",
    "})\n",
    "\n",
    "# 4) today target\n",
    "upsert(\"daily_targets\", f\"{USER}|{date_ist}\", {\n",
    "  \"_id\": f\"{USER}|{date_ist}\",\n",
    "  \"user\": USER,\n",
    "  \"date_ist\": date_ist,\n",
    "  \"target_pomos\": 6,\n",
    "  \"target_minutes\": 150,\n",
    "  \"source\": \"derived_from_weekly_plan\",\n",
    "  \"created_at\": UTCNOW, \"updated_at\": UTCNOW,\n",
    "  \"schema_version\": 1\n",
    "})\n",
    "\n",
    "# 5) a few sessions (work/break + activity)\n",
    "def ist_dt(h, m):  # today at HH:MM IST\n",
    "    dt = now_ist.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(hours=h, minutes=m)\n",
    "    return dt\n",
    "\n",
    "sessions = []\n",
    "# Work 1 (goal learn) 06:10‚Äì06:35\n",
    "sessions += [{\n",
    "  \"_id\": f\"{USER}|{date_ist}|W|{int(ist_dt(6,10).timestamp())}|25\",\n",
    "  \"user\": USER, \"date_ist\": date_ist, \"week_key\": week_key,\n",
    "  \"t\":\"W\",\"kind\":\"focus\",\"activity_type\": None,\"intensity\": None,\n",
    "  \"dur_min\":25,\"pom_equiv\":1.0,\n",
    "  \"started_at_ist\": ist_dt(6,10).astimezone(timezone.utc),\n",
    "  \"ended_at_ist\":   ist_dt(6,35).astimezone(timezone.utc),\n",
    "  \"deep_work\": True,\"context_switch\": False,\n",
    "  \"goal_mode\":\"weekly\",\"goal_id\":\"g_learn\",\"task\": None,\"cat\":\"Learning\",\n",
    "  \"alloc_bucket\":\"current\",\"break_autostart\": True,\"skipped\": None,\n",
    "  \"post_checkin\":{\"quality_1to5\":4,\"mood_1to5\":3,\"energy_1to5\":4,\"distraction\":None,\"note\":\"Good start\"},\n",
    "  \"device\":\"seed\",\"created_at\":UTCNOW,\"updated_at\":UTCNOW,\"schema_version\":1\n",
    "}]\n",
    "# Break 5\n",
    "sessions += [{\n",
    "  \"_id\": f\"{USER}|{date_ist}|B|{int(ist_dt(6,35).timestamp())}|5\",\n",
    "  \"user\": USER, \"date_ist\": date_ist, \"week_key\": week_key,\n",
    "  \"t\":\"B\",\"kind\": None,\"activity_type\": None,\"intensity\": None,\n",
    "  \"dur_min\":5,\"pom_equiv\":0.2,\n",
    "  \"started_at_ist\": ist_dt(6,35).astimezone(timezone.utc),\n",
    "  \"ended_at_ist\":   ist_dt(6,40).astimezone(timezone.utc),\n",
    "  \"deep_work\": None,\"context_switch\": None,\n",
    "  \"goal_mode\": None,\"goal_id\": None,\"task\": None,\"cat\": None,\n",
    "  \"alloc_bucket\": None,\"break_autostart\": None,\"skipped\": False,\n",
    "  \"post_checkin\": None,\"device\":\"seed\",\"created_at\":UTCNOW,\"updated_at\":UTCNOW,\"schema_version\":1\n",
    "}]\n",
    "# Work 2 (goal learn) 06:40‚Äì07:05\n",
    "sessions += [{\n",
    "  \"_id\": f\"{USER}|{date_ist}|W|{int(ist_dt(6,40).timestamp())}|25\",\n",
    "  \"user\": USER, \"date_ist\": date_ist, \"week_key\": week_key,\n",
    "  \"t\":\"W\",\"kind\":\"focus\",\"activity_type\": None,\"intensity\": None,\n",
    "  \"dur_min\":25,\"pom_equiv\":1.0,\n",
    "  \"started_at_ist\": ist_dt(6,40).astimezone(timezone.utc),\n",
    "  \"ended_at_ist\":   ist_dt(7,5).astimezone(timezone.utc),\n",
    "  \"deep_work\": True,\"context_switch\": False,\n",
    "  \"goal_mode\":\"weekly\",\"goal_id\":\"g_learn\",\"task\": None,\"cat\":\"Learning\",\n",
    "  \"alloc_bucket\":\"current\",\"break_autostart\": True,\"skipped\": None,\n",
    "  \"post_checkin\":{\"quality_1to5\":4,\"mood_1to5\":4,\"energy_1to5\":4,\"distraction\":\"none\",\"note\":None},\n",
    "  \"device\":\"seed\",\"created_at\":UTCNOW,\"updated_at\":UTCNOW,\"schema_version\":1\n",
    "}]\n",
    "# Activity: Meditation 10m (counts to Wellbeing), linked to g_well, 07:10‚Äì07:20\n",
    "sessions += [{\n",
    "  \"_id\": f\"{USER}|{date_ist}|W|{int(ist_dt(7,10).timestamp())}|10\",\n",
    "  \"user\": USER, \"date_ist\": date_ist, \"week_key\": week_key,\n",
    "  \"t\":\"W\",\"kind\":\"activity\",\"activity_type\":\"meditation\",\"intensity\":\"light\",\n",
    "  \"dur_min\":10,\"pom_equiv\":0.4,\n",
    "  \"started_at_ist\": ist_dt(7,10).astimezone(timezone.utc),\n",
    "  \"ended_at_ist\":   ist_dt(7,20).astimezone(timezone.utc),\n",
    "  \"deep_work\": None,\"context_switch\": None,\n",
    "  \"goal_mode\":\"weekly\",\"goal_id\":\"g_well\",\"task\": None,\"cat\":\"Health\",\n",
    "  \"alloc_bucket\":\"current\",\"break_autostart\": None,\"skipped\": None,\n",
    "  \"post_checkin\":{\"quality_1to5\":5,\"mood_1to5\":4,\"energy_1to5\":4,\"distraction\":None,\"note\":\"Calm\"},\n",
    "  \"device\":\"seed\",\"created_at\":UTCNOW,\"updated_at\":UTCNOW,\"schema_version\":1\n",
    "}]\n",
    "# Work 3 (goal proj) 08:00‚Äì08:25\n",
    "sessions += [{\n",
    "  \"_id\": f\"{USER}|{date_ist}|W|{int(ist_dt(8,0).timestamp())}|25\",\n",
    "  \"user\": USER, \"date_ist\": date_ist, \"week_key\": week_key,\n",
    "  \"t\":\"W\",\"kind\":\"focus\",\"activity_type\": None,\"intensity\": None,\n",
    "  \"dur_min\":25,\"pom_equiv\":1.0,\n",
    "  \"started_at_ist\": ist_dt(8,0).astimezone(timezone.utc),\n",
    "  \"ended_at_ist\":   ist_dt(8,25).astimezone(timezone.utc),\n",
    "  \"deep_work\": True,\"context_switch\": False,\n",
    "  \"goal_mode\":\"weekly\",\"goal_id\":\"g_proj\",\"task\": None,\"cat\":\"Projects\",\n",
    "  \"alloc_bucket\":\"current\",\"break_autostart\": True,\"skipped\": None,\n",
    "  \"post_checkin\":{\"quality_1to5\":3,\"mood_1to5\":3,\"energy_1to5\":3,\"distraction\":\"tabs\",\"note\":\"okay\"},\n",
    "  \"device\":\"seed\",\"created_at\":UTCNOW,\"updated_at\":UTCNOW,\"schema_version\":1\n",
    "}]\n",
    "\n",
    "for s in sessions:\n",
    "    db.sessions.update_one({\"_id\": s[\"_id\"]}, {\"$setOnInsert\": s}, upsert=True)\n",
    "\n",
    "# 6) reflection\n",
    "upsert(\"reflections\", f\"{USER}|{date_ist}\", {\n",
    "  \"_id\": f\"{USER}|{date_ist}\",\n",
    "  \"user\": USER,\n",
    "  \"date_ist\": date_ist,\n",
    "  \"focus_quality_1to5\": 4,\n",
    "  \"alignment_1to5\": 3,\n",
    "  \"blockers\": \"Meetings later\",\n",
    "  \"insights\": \"Early start helps\",\n",
    "  \"gratitude\": \"Family\",\n",
    "  \"notes\": [{\"at\": UTCNOW, \"text\": \"Idea: blog outline\", \"goal_id\": \"g_proj\"}],\n",
    "  \"reflection_submitted\": True,\n",
    "  \"created_at\": UTCNOW, \"updated_at\": UTCNOW,\n",
    "  \"schema_version\": 1\n",
    "})\n",
    "\n",
    "print(\"[seed] done:\", {\n",
    "  \"user\": USER, \"date_ist\": date_ist, \"week_key\": week_key,\n",
    "  \"plan_id\": f\"{USER}|{week_key}\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "642b2292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cfg] DB=Focus_DB USER=prashanth DRY_RUN=True\n",
      "Collections: daily_targets, goals, reflections, sessions, users, weekly_plans\n",
      "\n",
      "[before] per-collection user-doc counts\n",
      "  sessions       : 5\n",
      "  weekly_plans   : 1\n",
      "  daily_targets  : 1\n",
      "  reflections    : 1\n",
      "\n",
      "[dry-run] No deletes performed. Set DRY_RUN=false to apply.\n",
      "\n",
      "[after] per-collection user-doc counts\n",
      "  sessions       : 5\n",
      "  weekly_plans   : 1\n",
      "  daily_targets  : 1\n",
      "  reflections    : 1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Reset seeded data for a single user, keeping the user profile and goals.\n",
    "\n",
    "Env:\n",
    "  MONGO_URI   (required)\n",
    "  DB_NAME     (default: Focus_DB)\n",
    "  USER_ID     (default: prashanth)\n",
    "  DRY_RUN     (default: true)  -> set to \"false\" to actually delete\n",
    "\"\"\"\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pymongo import MongoClient\n",
    "import certifi\n",
    "\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\", \"mongodb+srv://prashanth01071995:pradsml%402025@cluster0.fsbic.mongodb.net/\")\n",
    "DB_NAME = os.environ.get(\"DB_NAME\", \"Focus_DB\")\n",
    "USER_ID   = os.getenv(\"USER_ID\", \"prashanth\")\n",
    "DRY_RUN   = (os.getenv(\"DRY_RUN\", \"true\").lower() != \"false\")\n",
    "\n",
    "if not MONGO_URI:\n",
    "    raise SystemExit(\"MONGO_URI is required\")\n",
    "\n",
    "client = MongoClient(MONGO_URI, tlsCAFile=certifi.where(), serverSelectionTimeoutMS=8000)\n",
    "client.admin.command(\"ping\")\n",
    "db = client[DB_NAME]\n",
    "\n",
    "collections = [\"sessions\", \"weekly_plans\", \"daily_targets\", \"reflections\"]\n",
    "\n",
    "print(f\"[cfg] DB={DB_NAME} USER={USER_ID} DRY_RUN={DRY_RUN}\")\n",
    "print(\"Collections:\", \", \".join(sorted(db.list_collection_names())))\n",
    "\n",
    "def count_all():\n",
    "    return {c: db[c].count_documents({\"user\": USER_ID}) for c in collections}\n",
    "\n",
    "before = count_all()\n",
    "print(\"\\n[before] per-collection user-doc counts\")\n",
    "for c, n in before.items():\n",
    "    print(f\"  {c:14} : {n}\")\n",
    "\n",
    "if DRY_RUN:\n",
    "    print(\"\\n[dry-run] No deletes performed. Set DRY_RUN=false to apply.\")\n",
    "else:\n",
    "    total_deleted = 0\n",
    "    for c in collections:\n",
    "        res = db[c].delete_many({\"user\": USER_ID})\n",
    "        print(f\"[deleted] {c:14} : {res.deleted_count}\")\n",
    "        total_deleted += res.deleted_count\n",
    "    # Optional: also clear any old logs/user_days if they exist in your DB\n",
    "    for maybe in [\"logs\", \"user_days\", \"daily_rhythm\"]:\n",
    "        if maybe in db.list_collection_names():\n",
    "            res = db[maybe].delete_many({\"user\": USER_ID})\n",
    "            if res.deleted_count:\n",
    "                print(f\"[deleted] {maybe:14} : {res.deleted_count}\")\n",
    "\n",
    "    print(f\"\\n[done] Total deleted: {total_deleted} docs @ {datetime.now(timezone.utc).isoformat()}Z\")\n",
    "\n",
    "after = count_all()\n",
    "print(\"\\n[after] per-collection user-doc counts\")\n",
    "for c, n in after.items():\n",
    "    print(f\"  {c:14} : {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a70d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4028bec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8941aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c07e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rachana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
